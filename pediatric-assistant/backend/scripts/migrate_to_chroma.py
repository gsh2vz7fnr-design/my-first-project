#!/usr/bin/env python3
"""
æ•°æ®è¿ç§»è„šæœ¬ - å°† JSON çŸ¥è¯†åº“è¿ç§»åˆ° ChromaDB

åŠŸèƒ½ï¼š
1. è¯»å– backend/data/knowledge_base/ ç›®å½•ä¸‹çš„æ‰€æœ‰ JSON æ–‡ä»¶
2. è½¬æ¢ä¸ºç»Ÿä¸€çš„ Document å¯¹è±¡
3. æ‰¹é‡å†™å…¥ ChromaDB
4. æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€é‡ç½®å’ŒéªŒè¯

ç”¨æ³•ï¼š
    # åŸºæœ¬ç”¨æ³•ï¼ˆå¢é‡è¿ç§»ï¼‰
    python scripts/migrate_to_chroma.py

    # é‡ç½®å¹¶è¿ç§»ï¼ˆæ¸…ç©ºæ—§æ•°æ®ï¼‰
    python scripts/migrate_to_chroma.py --reset

    # éªŒè¯æ¨¡å¼ï¼ˆä¸å†™å…¥ï¼Œåªæ£€æŸ¥æ•°æ®ï¼‰
    python scripts/migrate_to_chroma.py --dry-run

    # è¿ç§»å¹¶éªŒè¯
    python scripts/migrate_to_chroma.py --verify

    # ä»ä¸­æ–­å¤„ç»§ç»­
    python scripts/migrate_to_chroma.py --resume

    # è‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°
    python scripts/migrate_to_chroma.py --batch-size 50

é€‰é¡¹ï¼š
    --dry-run       ä»…éªŒè¯æ•°æ®ï¼Œä¸å®é™…å†™å…¥
    --reset         è¿ç§»å‰æ¸…ç©ºæ—§çš„ Collection
    --verify        è¿ç§»å®ŒæˆåéšæœºæŠ½å– 5 æ¡æ•°æ®éªŒè¯
    --resume        ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    --batch-size    è‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ 100ï¼‰
"""
import argparse
import asyncio
import json
import logging
import random
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Set

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# æ£€æŸ¥ tqdm æ˜¯å¦å®‰è£…
try:
    from tqdm import tqdm
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… tqdm åº“")
    print("è¯·è¿è¡Œ: pip install tqdm")
    sys.exit(1)

from app.config import settings
from app.services.vector_store import (
    ChromaStore,
    Document,
    VectorStoreError
)


# é…ç½®é”™è¯¯æ—¥å¿—
def setup_error_logger(log_file: str = "migration_errors.log") -> logging.Logger:
    """è®¾ç½®é”™è¯¯æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger("migration_errors")
    logger.setLevel(logging.ERROR)

    # æ–‡ä»¶å¤„ç†å™¨
    fh = logging.FileHandler(log_file, encoding='utf-8')
    fh.setLevel(logging.ERROR)

    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    fh.setFormatter(formatter)

    logger.addHandler(fh)
    return logger


error_logger = setup_error_logger()


class MigrationState:
    """è¿ç§»çŠ¶æ€ç®¡ç†ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""

    def __init__(self, state_file: Path):
        self.state_file = state_file
        self.migrated_files: Set[str] = set()
        self.migrated_ids: Set[str] = set()
        self._load()

    def _load(self) -> None:
        """åŠ è½½çŠ¶æ€"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.migrated_files = set(data.get('files', []))
                    self.migrated_ids = set(data.get('ids', []))
                print(f"åŠ è½½è¿ç§»çŠ¶æ€: {len(self.migrated_files)} ä¸ªæ–‡ä»¶, {len(self.migrated_ids)} æ¡è®°å½•")
            except Exception as e:
                print(f"è­¦å‘Š: åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def save(self) -> None:
        """ä¿å­˜çŠ¶æ€"""
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump({
                'files': list(self.migrated_files),
                'ids': list(self.migrated_ids),
                'updated_at': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)

    def mark_file_done(self, filename: str) -> None:
        """æ ‡è®°æ–‡ä»¶å·²å®Œæˆ"""
        self.migrated_files.add(filename)

    def is_file_done(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å®Œæˆ"""
        return filename in self.migrated_files

    def add_ids(self, ids: List[str]) -> None:
        """æ·»åŠ å·²è¿ç§»çš„ ID"""
        self.migrated_ids.update(ids)

    def clear(self) -> None:
        """æ¸…é™¤çŠ¶æ€"""
        self.migrated_files.clear()
        self.migrated_ids.clear()
        if self.state_file.exists():
            self.state_file.unlink()


class KnowledgeBaseMigrator:
    """çŸ¥è¯†åº“è¿ç§»å™¨"""

    # Embedding å¤±è´¥é‡è¯•é…ç½®
    MAX_RETRIES = 3
    RETRY_DELAY = 2.0  # ç§’

    def __init__(
        self,
        knowledge_base_path: Path,
        persist_directory: Path,
        collection_name: str = "pediatric_knowledge_base",
        batch_size: int = 100,
        dry_run: bool = False,
        resume: bool = False,
        reset: bool = False
    ):
        self.knowledge_base_path = knowledge_base_path
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.batch_size = batch_size
        self.dry_run = dry_run
        self.reset = reset

        # çŠ¶æ€æ–‡ä»¶
        self.state = MigrationState(
            persist_directory / ".migration_state.json"
        ) if resume else None

        # å¦‚æœæ˜¯é‡ç½®æ¨¡å¼ï¼Œæ¸…é™¤çŠ¶æ€
        if reset and self.state:
            self.state.clear()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_files': 0,
            'total_entries': 0,
            'migrated_entries': 0,
            'skipped_entries': 0,
            'failed_entries': 0,
            'retry_count': 0,
            'start_time': None,
            'end_time': None
        }

        # å­˜å‚¨æ‰€æœ‰æ–‡æ¡£ IDï¼ˆç”¨äºéªŒè¯ï¼‰
        self._all_doc_ids: List[str] = []

        # å‘é‡å­˜å‚¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._store: Optional[ChromaStore] = None

    async def _get_store(self) -> ChromaStore:
        """è·å–å‘é‡å­˜å‚¨å®ä¾‹"""
        if self._store is None:
            self._store = ChromaStore(
                collection_name=self.collection_name,
                persist_directory=str(self.persist_directory),
            )
            await self._store._ensure_initialized()
        return self._store

    def _load_json_file(self, filepath: Path) -> List[Dict[str, Any]]:
        """
        åŠ è½½å•ä¸ª JSON æ–‡ä»¶

        Args:
            filepath: JSON æ–‡ä»¶è·¯å¾„

        Returns:
            List[Dict]: æ¡ç›®åˆ—è¡¨

        Raises:
            json.JSONDecodeError: JSON è§£æé”™è¯¯
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        entries = []
        file_topic = data.get('topic', '')
        file_category = data.get('category', '')
        file_source = data.get('source', '')

        for entry in data.get('entries', []):
            # åˆå¹¶æ–‡ä»¶çº§åˆ«çš„å…ƒæ•°æ®
            enriched_entry = {
                **entry,
                'topic': entry.get('topic') or file_topic,
                'category': entry.get('category') or file_category,
                'source': entry.get('source') or file_source,
            }
            entries.append(enriched_entry)

        return entries

    def _entry_to_document(self, entry: Dict[str, Any]) -> Optional[Document]:
        """
        å°†çŸ¥è¯†åº“æ¡ç›®è½¬æ¢ä¸º Document å¯¹è±¡

        Args:
            entry: çŸ¥è¯†åº“æ¡ç›®

        Returns:
            Document: æ–‡æ¡£å¯¹è±¡ï¼Œè½¬æ¢å¤±è´¥è¿”å› None
        """
        try:
            # æ„å»ºå…ƒæ•°æ®ï¼ˆæŒ‰ Schema è¦æ±‚ï¼‰
            metadata = {
                'id': entry.get('id', ''),
                'title': entry.get('title', ''),
                'source': entry.get('source', ''),
                'topic': entry.get('topic', ''),
                'category': entry.get('category', ''),
                'alert_level': entry.get('alert_level', ''),
                'tags': ','.join(entry.get('tags', [])) if isinstance(entry.get('tags'), list) else (entry.get('tags', '') or ''),
                'h1': entry.get('h1', ''),
                'h2': entry.get('h2', ''),
                'source_file': entry.get('source_file', ''),
                'page_range': entry.get('page_range', ''),
                'token_count': entry.get('token_count', 0),
            }

            # å¤„ç†å¹´é¾„èŒƒå›´ï¼ˆè½¬æ¢ä¸ºæ•°å€¼ä¾¿äºè¿‡æ»¤ï¼‰
            age_range = entry.get('age_range', '')
            if age_range:
                try:
                    if '-' in str(age_range) and 'ä¸ªæœˆ' in str(age_range):
                        parts = str(age_range).replace('ä¸ªæœˆ', '').split('-')
                        metadata['age_range_min'] = int(parts[0])
                        metadata['age_range_max'] = int(parts[1])
                        metadata['age_range'] = age_range  # ä¿ç•™åŸå§‹å­—ç¬¦ä¸²
                except (ValueError, IndexError) as e:
                    error_logger.error(f"è§£æå¹´é¾„èŒƒå›´å¤±è´¥: {age_range}, error: {e}")

            content = entry.get('content', '')
            if not content:
                error_logger.error(f"æ–‡æ¡£å†…å®¹ä¸ºç©º: {entry.get('id', 'unknown')}")
                return None

            return Document(
                id=entry.get('id', f"auto_{hash(content) % 10000000}"),
                content=content,
                metadata=metadata
            )

        except Exception as e:
            error_logger.error(f"è½¬æ¢æ–‡æ¡£å¤±è´¥: {entry.get('id', 'unknown')}, error: {e}")
            return None

    async def _add_documents_with_retry(
        self,
        store: ChromaStore,
        documents: List[Document]
    ) -> int:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æ‰¹é‡æ·»åŠ æ–‡æ¡£

        Args:
            store: å‘é‡å­˜å‚¨å®ä¾‹
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            int: æˆåŠŸæ·»åŠ çš„æ–‡æ¡£æ•°é‡
        """
        last_error = None

        for attempt in range(self.MAX_RETRIES):
            try:
                count = await store.add_documents(documents)
                return count

            except VectorStoreError as e:
                last_error = e
                self.stats['retry_count'] += 1

                if attempt < self.MAX_RETRIES - 1:
                    wait_time = self.RETRY_DELAY * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    error_logger.error(
                        f"æ‰¹é‡å†™å…¥å¤±è´¥ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}), "
                        f"{wait_time}s åé‡è¯•: {e}"
                    )
                    print(f"\nâš ï¸ å†™å…¥å¤±è´¥ï¼Œ{wait_time}s åé‡è¯• ({attempt + 1}/{self.MAX_RETRIES})...")
                    await asyncio.sleep(wait_time)
                else:
                    error_logger.error(f"æ‰¹é‡å†™å…¥æœ€ç»ˆå¤±è´¥: {e}")

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        print(f"\nâŒ æ‰¹æ¬¡å†™å…¥å¤±è´¥ï¼ˆå·²é‡è¯• {self.MAX_RETRIES} æ¬¡ï¼‰: {last_error}")
        return 0

    async def migrate(self) -> bool:
        """
        æ‰§è¡Œè¿ç§»

        Returns:
            bool: è¿ç§»æ˜¯å¦æˆåŠŸ
        """
        self.stats['start_time'] = time.time()

        # 1. æ”¶é›†æ‰€æœ‰ JSON æ–‡ä»¶
        json_files = list(self.knowledge_base_path.glob('*.json'))
        self.stats['total_files'] = len(json_files)

        if not json_files:
            print(f"é”™è¯¯: æœªæ‰¾åˆ° JSON æ–‡ä»¶ ({self.knowledge_base_path})")
            return False

        print("ChromaDB æ•°æ®è¿ç§»è„šæœ¬")
        print("=" * 50)
        print(f"çŸ¥è¯†åº“è·¯å¾„:   {self.knowledge_base_path}")
        print(f"æŒä¹…åŒ–ç›®å½•:   {self.persist_directory}")
        print(f"é›†åˆåç§°:     {self.collection_name}")
        print(f"æ‰¾åˆ°æ–‡ä»¶:     {len(json_files)} ä¸ª")
        print(f"æ‰¹æ¬¡å¤§å°:     {self.batch_size}")
        print(f"æ¨¡å¼:         {'éªŒè¯æ¨¡å¼ (dry-run)' if self.dry_run else 'è¿ç§»æ¨¡å¼'}")
        if self.state:
            print(f"æ–­ç‚¹ç»­ä¼ :     å¯ç”¨")
        print("=" * 50)
        print()

        # 2. åˆå§‹åŒ–å‘é‡å­˜å‚¨
        if not self.dry_run:
            store = await self._get_store()
            print(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
            print(f"å½“å‰æ–‡æ¡£æ•°: {store.count}")

            # é‡ç½®æ¨¡å¼ï¼šæ¸…ç©ºæ—§ Collection
            if self.reset:
                print("\nğŸ—‘ï¸  é‡ç½®æ¨¡å¼ï¼šæ¸…ç©ºæ—§ Collection...")
                await store.delete_collection()
                # é‡æ–°åˆå§‹åŒ–
                self._store = None
                store = await self._get_store()
                print(f"Collection å·²é‡ç½®ï¼Œå½“å‰æ–‡æ¡£æ•°: {store.count}")

            print()

        # 3. éå†æ–‡ä»¶ï¼Œè§£ææ–‡æ¡£
        all_documents: List[Document] = []
        parse_errors: int = 0

        for json_file in tqdm(json_files, desc="ğŸ“‚ è¯»å–æ–‡ä»¶"):
            # æ£€æŸ¥æ˜¯å¦å·²è¿ç§»ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            if self.state and self.state.is_file_done(json_file.name):
                continue

            try:
                entries = self._load_json_file(json_file)
                self.stats['total_entries'] += len(entries)

                for entry in entries:
                    doc = self._entry_to_document(entry)
                    if doc:
                        all_documents.append(doc)
                        self._all_doc_ids.append(doc.id)
                    else:
                        parse_errors += 1
                        self.stats['failed_entries'] += 1

                # æ ‡è®°æ–‡ä»¶å·²è¯»å–
                if self.state:
                    self.state.mark_file_done(json_file.name)

            except json.JSONDecodeError as e:
                parse_errors += 1
                error_logger.error(f"JSON è§£æé”™è¯¯ ({json_file.name}): {e}")
            except Exception as e:
                parse_errors += 1
                error_logger.error(f"æ–‡ä»¶è¯»å–é”™è¯¯ ({json_file.name}): {e}")

        if parse_errors > 0:
            print(f"\nâš ï¸  è§£æè¿‡ç¨‹ä¸­å‘ç° {parse_errors} ä¸ªé”™è¯¯ï¼Œå·²è®°å½•åˆ° migration_errors.log")

        print(f"\nğŸ“‹ å…±è§£æ {len(all_documents)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")

        # 4. dry-run æ¨¡å¼ï¼šæ˜¾ç¤ºæ ·ä¾‹åé€€å‡º
        if self.dry_run:
            print("\nğŸ“ æ•°æ®æ ·ä¾‹ (å‰ 3 æ¡):")
            print("-" * 50)
            for i, doc in enumerate(all_documents[:3], 1):
                print(f"\næ–‡æ¡£ {i}:")
                print(f"  ID: {doc.id}")
                print(f"  æ ‡é¢˜: {doc.metadata.get('title', 'N/A')}")
                print(f"  åˆ†ç±»: {doc.metadata.get('category', 'N/A')}")
                print(f"  å†…å®¹: {doc.content[:100]}...")
                print(f"  å…ƒæ•°æ®: {list(doc.metadata.keys())}")

            print("\nâœ… éªŒè¯æ¨¡å¼å®Œæˆï¼Œæ•°æ®æ ¼å¼æ­£ç¡®ï¼")
            self.stats['end_time'] = time.time()
            self._print_summary()
            return True

        # 5. æ‰¹é‡å†™å…¥
        if not all_documents:
            print("âš ï¸  æ²¡æœ‰éœ€è¦è¿ç§»çš„æ–‡æ¡£")
            return True

        store = await self._get_store()

        # åˆ†æ‰¹
        batches = [
            all_documents[i:i + self.batch_size]
            for i in range(0, len(all_documents), self.batch_size)
        ]

        print(f"\nğŸš€ å¼€å§‹å†™å…¥ {len(batches)} ä¸ªæ‰¹æ¬¡...")
        print()

        with tqdm(total=len(all_documents), desc="ğŸ“ å†™å…¥æ–‡æ¡£", unit="æ¡") as pbar:
            for batch_idx, batch in enumerate(batches, 1):
                count = await self._add_documents_with_retry(store, batch)

                if count > 0:
                    self.stats['migrated_entries'] += count
                    pbar.update(len(batch))

                    # æ›´æ–°çŠ¶æ€ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
                    if self.state:
                        self.state.add_ids([doc.id for doc in batch])
                        # æ¯ 5 ä¸ªæ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡
                        if batch_idx % 5 == 0:
                            self.state.save()
                else:
                    self.stats['failed_entries'] += len(batch)
                    pbar.update(len(batch))  # å³ä½¿å¤±è´¥ä¹Ÿæ›´æ–°è¿›åº¦æ¡

        # æœ€ç»ˆä¿å­˜çŠ¶æ€
        if self.state:
            self.state.save()

        self.stats['end_time'] = time.time()

        # 6. æ‰“å°ç»Ÿè®¡
        self._print_summary()

        return self.stats['failed_entries'] == 0

    async def verify(self, sample_size: int = 5) -> bool:
        """
        éªŒè¯è¿ç§»ç»“æœï¼ˆéšæœºæŠ½å–æ ·æœ¬ï¼‰

        Args:
            sample_size: æŠ½æ ·æ•°é‡

        Returns:
            bool: éªŒè¯æ˜¯å¦é€šè¿‡
        """
        store = await self._get_store()

        print(f"\nğŸ” éªŒè¯è¿ç§»ç»“æœ...")
        print(f"æ€»æ–‡æ¡£æ•°: {store.count}")
        print()

        all_passed = True

        # 1. éšæœºæŠ½å– ID éªŒè¯ï¼ˆå¦‚æœæœ‰è®°å½•ï¼‰
        if self._all_doc_ids and len(self._all_doc_ids) >= sample_size:
            sample_ids = random.sample(self._all_doc_ids, sample_size)

            print(f"ğŸ“‹ éšæœºæŠ½æ ·éªŒè¯ ({sample_size} æ¡):")
            print("-" * 40)

            for i, doc_id in enumerate(sample_ids, 1):
                doc = await store.get_document_by_id(doc_id)
                if doc:
                    print(f"  âœ… [{i}] ID: {doc_id}")
                    print(f"      æ ‡é¢˜: {doc.metadata.get('title', 'N/A')[:30]}...")
                else:
                    print(f"  âŒ [{i}] ID: {doc_id} - æœªæ‰¾åˆ°")
                    all_passed = False
                    error_logger.error(f"éªŒè¯å¤±è´¥: æ–‡æ¡£ {doc_id} æœªæ‰¾åˆ°")

        print()

        # 2. æœç´¢åŠŸèƒ½éªŒè¯
        print("ğŸ” æœç´¢åŠŸèƒ½éªŒè¯:")
        print("-" * 40)

        test_queries = [
            "å‘çƒ§æ€ä¹ˆåŠ",
            "è…¹æ³»",
            "å’³å—½",
            "æ³°è¯ºæ—",
            "ç¾æ—",
        ]

        for query in test_queries:
            try:
                results = await store.search(query, top_k=3)
                if results:
                    top_score = results[0].score
                    print(f"  âœ… '{query}': {len(results)} æ¡ç»“æœ (Top-1 score: {top_score:.3f})")
                else:
                    print(f"  âš ï¸  '{query}': æ— ç»“æœ")
            except Exception as e:
                print(f"  âŒ '{query}': å¤±è´¥ - {e}")
                all_passed = False
                error_logger.error(f"æœç´¢éªŒè¯å¤±è´¥: {query}, error: {e}")

        return all_passed

    def _print_summary(self) -> None:
        """æ‰“å°è¿ç§»æ‘˜è¦"""
        duration = self.stats['end_time'] - self.stats['start_time']

        print("\n" + "=" * 50)
        print("ğŸ“Š è¿ç§»æ‘˜è¦")
        print("=" * 50)
        print(f"æ€»æ–‡ä»¶æ•°:       {self.stats['total_files']}")
        print(f"æ€»æ¡ç›®æ•°:       {self.stats['total_entries']}")
        print(f"å·²è¿ç§»:         {self.stats['migrated_entries']}")
        print(f"è·³è¿‡:           {self.stats['skipped_entries']}")
        print(f"å¤±è´¥:           {self.stats['failed_entries']}")
        print(f"é‡è¯•æ¬¡æ•°:       {self.stats['retry_count']}")
        print("-" * 50)
        print(f"è€—æ—¶:           {duration:.2f} ç§’")
        if duration > 0 and self.stats['migrated_entries'] > 0:
            rate = self.stats['migrated_entries'] / duration
            print(f"é€Ÿç‡:           {rate:.1f} æ¡/ç§’")
        print("=" * 50)


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å°† JSON çŸ¥è¯†åº“è¿ç§»åˆ° ChromaDB',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python scripts/migrate_to_chroma.py                    # åŸºæœ¬è¿ç§»
  python scripts/migrate_to_chroma.py --reset            # é‡ç½®å¹¶è¿ç§»
  python scripts/migrate_to_chroma.py --dry-run          # éªŒè¯æ¨¡å¼
  python scripts/migrate_to_chroma.py --verify           # è¿ç§»å¹¶éªŒè¯
  python scripts/migrate_to_chroma.py --resume           # æ–­ç‚¹ç»­ä¼ 
        """
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ä»…éªŒè¯æ•°æ®ï¼Œä¸å®é™…å†™å…¥'
    )
    parser.add_argument(
        '--reset',
        action='store_true',
        help='è¿ç§»å‰æ¸…ç©ºæ—§çš„ Collection'
    )
    parser.add_argument(
        '--verify',
        action='store_true',
        help='è¿ç§»å®ŒæˆåéšæœºæŠ½å– 5 æ¡æ•°æ®éªŒè¯'
    )
    parser.add_argument(
        '--resume',
        action='store_true',
        help='ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=100,
        help='æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ 100ï¼‰'
    )
    parser.add_argument(
        '--knowledge-base',
        type=str,
        default=None,
        help='çŸ¥è¯†åº“è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰'
    )
    parser.add_argument(
        '--persist-dir',
        type=str,
        default=None,
        help='æŒä¹…åŒ–ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰'
    )

    args = parser.parse_args()

    # ç¡®å®šè·¯å¾„
    backend_dir = Path(__file__).parent.parent
    knowledge_base_path = Path(args.knowledge_base) if args.knowledge_base else Path(settings.KNOWLEDGE_BASE_PATH)
    persist_dir = Path(args.persist_dir) if args.persist_dir else Path(settings.VECTOR_DB_PATH)

    # æ£€æŸ¥çŸ¥è¯†åº“è·¯å¾„
    if not knowledge_base_path.exists():
        print(f"âŒ é”™è¯¯: çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨: {knowledge_base_path}")
        sys.exit(1)

    # åˆ›å»ºè¿ç§»å™¨
    migrator = KnowledgeBaseMigrator(
        knowledge_base_path=knowledge_base_path,
        persist_directory=persist_dir,
        collection_name="pediatric_knowledge_base",
        batch_size=args.batch_size,
        dry_run=args.dry_run,
        resume=args.resume,
        reset=args.reset
    )

    # æ‰§è¡Œè¿ç§»
    success = await migrator.migrate()

    # éªŒè¯
    if args.verify and success and not args.dry_run:
        verify_success = await migrator.verify()
        if not verify_success:
            print("\nâš ï¸  éªŒè¯å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ migration_errors.log")
            sys.exit(1)

    if success:
        print("\nâœ… è¿ç§»å®Œæˆ!")
    else:
        print("\nâŒ è¿ç§»è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥ migration_errors.log")
        sys.exit(1)


if __name__ == '__main__':
    asyncio.run(main())
