"""
å¯¹è¯è·¯ç”±
"""
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from loguru import logger
import json
import asyncio
import uuid
from typing import AsyncGenerator, Optional, List, Dict

from app.models.user import ChatRequest, StreamChunk
from app.services.llm_service import llm_service
from app.services.triage_engine import triage_engine
from app.services.safety_filter import safety_filter
from app.services.stream_filter import StreamSafetyFilter
from app.services.rag_service import rag_service
from app.config import settings
from app.services.profile_service import profile_service
from app.services.conversation_service import conversation_service

router = APIRouter()


@router.post("/send")
async def send_message(request: ChatRequest):
    """
    å‘é€æ¶ˆæ¯ï¼ˆéæµå¼ï¼‰

    Args:
        request: èŠå¤©è¯·æ±‚

    Returns:
        dict: å“åº”ç»“æœ
    """
    try:
        # 0. åŠ è½½ç”¨æˆ·æ¡£æ¡ˆ
        profile = profile_service.get_profile(request.user_id)
        context = {
            "baby_info": profile.baby_info.model_dump(),
            "allergy_history": [x.model_dump() for x in profile.allergy_history],
            "medical_history": [x.model_dump() for x in profile.medical_history]
        }

        # 1. æ£€æŸ¥å¤„æ–¹æ„å›¾
        if safety_filter.check_prescription_intent(request.message):
            conversation_id = request.conversation_id or f"conv_{uuid.uuid4().hex[:12]}"
            conversation_service.append_message(conversation_id, request.user_id, "user", request.message)
            return {
                "code": 0,
                "data": {
                    "conversation_id": conversation_id,
                    "message": safety_filter.get_prescription_refusal_message(),
                    "sources": [],
                    "metadata": {"blocked": True, "reason": "prescription_intent"}
                }
            }

        # 2. æå–æ„å›¾å’Œå®ä½“
        intent_result = await llm_service.extract_intent_and_entities(
            user_input=request.message,
            context=context
        )

        logger.info(f"æ„å›¾è¯†åˆ«ç»“æœ: {intent_result}")

        # 2.5a é—®å€™/é—²èŠè·¯ç”±
        if intent_result.intent.type == "greeting":
            conversation_id = request.conversation_id or f"conv_{uuid.uuid4().hex[:12]}"
            greeting_reply = (
                "æ‚¨å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½å„¿ç§‘åŠ©æ‰‹ ğŸ‘‹\n\n"
                "æˆ‘å¯ä»¥å¸®æ‚¨ï¼š\n"
                "â€¢ è¯„ä¼°å®å®çš„ç—‡çŠ¶ï¼ˆå‘çƒ§ã€å’³å—½ã€è…¹æ³»ç­‰ï¼‰\n"
                "â€¢ æä¾›ç§‘å­¦çš„å±…å®¶æŠ¤ç†å»ºè®®\n"
                "â€¢ åˆ¤æ–­æ˜¯å¦éœ€è¦å°±åŒ»\n\n"
                "è¯·æè¿°å®å®çš„æƒ…å†µï¼Œä¾‹å¦‚ï¼šã€Œå®å®8ä¸ªæœˆï¼Œå‘çƒ§38.5åº¦ï¼Œç²¾ç¥ä¸å¥½ã€"
            )
            greeting_reply = safety_filter.add_disclaimer(greeting_reply)
            conversation_service.append_message(conversation_id, request.user_id, "user", request.message)
            conversation_service.append_message(conversation_id, request.user_id, "assistant", greeting_reply)
            return {
                "code": 0,
                "data": {
                    "conversation_id": conversation_id,
                    "message": greeting_reply,
                    "sources": [],
                    "metadata": {"intent": "greeting"}
                }
            }

        # 2.5b Slot-filling è·¯ç”±
        if intent_result.intent.type == "slot_filling":
            conversation_id = request.conversation_id or f"conv_{uuid.uuid4().hex[:12]}"
            conversation_service.append_message(conversation_id, request.user_id, "user", request.message)

            merged_entities = intent_result.entities.copy()
            if not merged_entities.get("symptom"):
                history = conversation_service.get_history(conversation_id, limit=10)
                recovered_symptom = _recover_symptom_from_history(history)
                if recovered_symptom:
                    merged_entities["symptom"] = recovered_symptom

            symptom = merged_entities.get("symptom", "")
            if not symptom:
                follow_up = "ä¸ºäº†ç»§ç»­åˆ†è¯Šï¼Œè¯·å…ˆå‘Šè¯‰æˆ‘å®å®çš„ä¸»è¦ç—‡çŠ¶ï¼ˆå¦‚å‘çƒ§ã€å’³å—½ã€è…¹æ³»ç­‰ï¼‰ã€‚"
                conversation_service.append_message(conversation_id, request.user_id, "assistant", follow_up)
                return {
                    "code": 0,
                    "data": {
                        "conversation_id": conversation_id,
                        "message": follow_up,
                        "sources": [],
                        "metadata": {
                            "intent": "slot_filling",
                            "need_follow_up": True,
                            "missing_slots": ["symptom"]
                        }
                    }
                }

            danger_alert = triage_engine.check_danger_signals(merged_entities)
            if danger_alert:
                conversation_service.append_message(conversation_id, request.user_id, "assistant", danger_alert)
                return {
                    "code": 0,
                    "data": {
                        "conversation_id": conversation_id,
                        "message": danger_alert,
                        "sources": [],
                        "metadata": {
                            "intent": "triage",
                            "origin_intent": "slot_filling",
                            "triage_level": "emergency",
                            "danger_signal": True
                        }
                    }
                }

            missing_slots = triage_engine.get_missing_slots(
                symptom,
                merged_entities,
                profile_context=context
            )

            if missing_slots:
                follow_up = triage_engine.generate_follow_up_question(symptom, missing_slots)
                conversation_service.append_message(conversation_id, request.user_id, "assistant", follow_up)
                return {
                    "code": 0,
                    "data": {
                        "conversation_id": conversation_id,
                        "message": follow_up,
                        "sources": [],
                        "metadata": {
                            "intent": "triage",
                            "origin_intent": "slot_filling",
                            "need_follow_up": True,
                            "missing_slots": missing_slots
                        }
                    }
                }

            decision = triage_engine.make_triage_decision(symptom, merged_entities)
            response_message = f"**{decision.reason}**\n\n{decision.action}"
            response_message = safety_filter.add_disclaimer(response_message)
            conversation_service.append_message(conversation_id, request.user_id, "assistant", response_message)

            return {
                "code": 0,
                "data": {
                    "conversation_id": conversation_id,
                    "message": response_message,
                    "sources": [],
                    "metadata": {
                        "intent": "triage",
                        "origin_intent": "slot_filling",
                        "triage_level": decision.level,
                        "entities": merged_entities
                    }
                }
            }

        # 3. å¦‚æœæ˜¯åˆ†è¯Šæ„å›¾ï¼Œè¿›è¡Œåˆ†è¯Š
        if intent_result.intent.type == "triage":
            conversation_id = request.conversation_id or f"conv_{uuid.uuid4().hex[:12]}"
            conversation_service.append_message(conversation_id, request.user_id, "user", request.message)
            # æ£€æŸ¥å±é™©ä¿¡å·
            danger_alert = triage_engine.check_danger_signals(intent_result.entities)
            if danger_alert:
                conversation_service.append_message(conversation_id, request.user_id, "assistant", danger_alert)
                return {
                    "code": 0,
                    "data": {
                        "conversation_id": conversation_id,
                        "message": danger_alert,
                        "sources": [],
                        "metadata": {
                            "intent": "triage",
                            "triage_level": "emergency",
                            "danger_signal": True
                        }
                    }
                }

            # æ£€æŸ¥ç¼ºå¤±æ§½ä½ï¼ˆä¼ å…¥æ¡£æ¡ˆä¸Šä¸‹æ–‡ä»¥æ”¯æŒè‡ªåŠ¨å¡«å……ï¼‰
            symptom = intent_result.entities.get("symptom", "")
            missing_slots = triage_engine.get_missing_slots(
                symptom,
                intent_result.entities,
                profile_context=context
            )

            if missing_slots:
                # éœ€è¦è¿½é—®
                follow_up = triage_engine.generate_follow_up_question(symptom, missing_slots)
                conversation_service.append_message(conversation_id, request.user_id, "assistant", follow_up)
                return {
                    "code": 0,
                    "data": {
                        "conversation_id": conversation_id,
                        "message": follow_up,
                        "sources": [],
                        "metadata": {
                            "intent": "triage",
                            "need_follow_up": True,
                            "missing_slots": missing_slots
                        }
                    }
                }

            # åšå‡ºåˆ†è¯Šå†³ç­–
            decision = triage_engine.make_triage_decision(symptom, intent_result.entities)

            response_message = f"**{decision.reason}**\n\n{decision.action}"
            response_message = safety_filter.add_disclaimer(response_message)
            conversation_service.append_message(conversation_id, request.user_id, "assistant", response_message)

            return {
                "code": 0,
                "data": {
                    "conversation_id": conversation_id,
                    "message": response_message,
                    "sources": [],
                    "metadata": {
                        "intent": "triage",
                        "triage_level": decision.level,
                        "entities": intent_result.entities
                    }
                }
            }

        # 4. å…¶ä»–æ„å›¾ï¼ˆå’¨è¯¢ã€ç”¨è¯ã€æŠ¤ç†ï¼‰- ä½¿ç”¨RAG
        # æ£€æµ‹æƒ…ç»ªå¹¶æ·»åŠ æƒ…ç»ªæ‰¿æ¥
        emotion_support = llm_service.detect_emotion(request.message)

        rag_result = await rag_service.generate_answer_with_sources(
            query=request.message,
            context=context
        )

        # å®‰å…¨è¿‡æ»¤
        safety_result = safety_filter.filter_output(rag_result.answer)
        if not safety_result.is_safe:
            conversation_id = request.conversation_id or f"conv_{uuid.uuid4().hex[:12]}"
            conversation_service.append_message(conversation_id, request.user_id, "user", request.message)
            conversation_service.append_message(conversation_id, request.user_id, "assistant", safety_result.fallback_message)
            return {
                "code": 0,
                "data": {
                    "conversation_id": conversation_id,
                    "message": safety_result.fallback_message,
                    "sources": [],
                    "metadata": {"blocked": True, "reason": "safety_filter"}
                }
            }

        # æ·»åŠ æƒ…ç»ªæ‰¿æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
        if emotion_support:
            response_message = f"{emotion_support}\n\n{rag_result.answer}"
        else:
            response_message = rag_result.answer

        # æ·»åŠ å…è´£å£°æ˜
        response_message = safety_filter.add_disclaimer(response_message)
        conversation_id = request.conversation_id or f"conv_{uuid.uuid4().hex[:12]}"
        conversation_service.append_message(conversation_id, request.user_id, "user", request.message)
        conversation_service.append_message(conversation_id, request.user_id, "assistant", response_message)

        # å®‰æ’å»¶è¿Ÿæ¡£æ¡ˆæå–ï¼ˆ30åˆ†é’Ÿåï¼‰
        asyncio.create_task(
            profile_service.schedule_delayed_extraction(
                user_id=request.user_id,
                conversation_id=conversation_id,
                delay_minutes=30
            )
        )

        # æ·»åŠ æ¥æºå…ƒæ•°æ®
        sources_metadata = rag_service.get_sources_metadata(rag_result.sources)
        return {
            "code": 0,
            "data": {
                "conversation_id": conversation_id,
                "message": response_message,
                "sources": sources_metadata,
                "metadata": {
                    "intent": intent_result.intent.type,
                    "has_source": rag_result.has_source,
                    "emotion_detected": emotion_support is not None
                }
            }
        }

    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/stream")
async def send_message_stream(request: ChatRequest):
    """
    å‘é€æ¶ˆæ¯ï¼ˆæµå¼ï¼‰

    Args:
        request: èŠå¤©è¯·æ±‚

    Returns:
        StreamingResponse: æµå¼å“åº”
    """
    async def generate() -> AsyncGenerator[str, None]:
        """ç”Ÿæˆæµå¼å“åº”"""
        try:
            conversation_id = request.conversation_id or f"conv_{request.user_id}_{int(asyncio.get_event_loop().time())}"

            # 0. åŠ è½½ç”¨æˆ·æ¡£æ¡ˆ
            profile = profile_service.get_profile(request.user_id)
            context = {
                "baby_info": profile.baby_info.model_dump(),
                "allergy_history": [x.model_dump() for x in profile.allergy_history],
                "medical_history": [x.model_dump() for x in profile.medical_history]
            }

            # 1. æ£€æŸ¥å¤„æ–¹æ„å›¾
            if safety_filter.check_prescription_intent(request.message):
                # Send metadata first
                metadata_chunk = StreamChunk(
                    type="metadata",
                    metadata={"blocked": True, "reason": "prescription_intent"}
                )
                yield f"data: {metadata_chunk.model_dump_json()}\n\n"

                chunk = StreamChunk(
                    type="content",
                    content=safety_filter.get_prescription_refusal_message()
                )
                yield f"data: {chunk.model_dump_json()}\n\n"
                yield "data: {\"type\": \"done\"}\n\n"
                return

            # 2. æå–æ„å›¾å’Œå®ä½“
            intent_result = await llm_service.extract_intent_and_entities(
                user_input=request.message,
                context=context
            )

            # 2.5a é—®å€™/é—²èŠè·¯ç”±
            if intent_result.intent.type == "greeting":
                greeting_reply = (
                    "æ‚¨å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½å„¿ç§‘åŠ©æ‰‹ ğŸ‘‹\n\n"
                    "æˆ‘å¯ä»¥å¸®æ‚¨ï¼š\n"
                    "â€¢ è¯„ä¼°å®å®çš„ç—‡çŠ¶ï¼ˆå‘çƒ§ã€å’³å—½ã€è…¹æ³»ç­‰ï¼‰\n"
                    "â€¢ æä¾›ç§‘å­¦çš„å±…å®¶æŠ¤ç†å»ºè®®\n"
                    "â€¢ åˆ¤æ–­æ˜¯å¦éœ€è¦å°±åŒ»\n\n"
                    "è¯·æè¿°å®å®çš„æƒ…å†µï¼Œä¾‹å¦‚ï¼šã€Œå®å®8ä¸ªæœˆï¼Œå‘çƒ§38.5åº¦ï¼Œç²¾ç¥ä¸å¥½ã€"
                )
                greeting_reply = safety_filter.add_disclaimer(greeting_reply)
                conversation_service.append_message(conversation_id, request.user_id, "user", request.message)
                conversation_service.append_message(conversation_id, request.user_id, "assistant", greeting_reply)

                metadata_chunk = StreamChunk(type="metadata", metadata={"intent": "greeting"})
                yield f"data: {metadata_chunk.model_dump_json()}\n\n"

                chunk = StreamChunk(type="content", content=greeting_reply)
                yield f"data: {chunk.model_dump_json()}\n\n"
                yield "data: {\"type\": \"done\"}\n\n"
                return

            # 2.5b Slot-filling è·¯ç”±
            if intent_result.intent.type == "slot_filling":
                conversation_service.append_message(conversation_id, request.user_id, "user", request.message)

                merged_entities = intent_result.entities.copy()
                if not merged_entities.get("symptom"):
                    history = conversation_service.get_history(conversation_id, limit=10)
                    recovered_symptom = _recover_symptom_from_history(history)
                    if recovered_symptom:
                        merged_entities["symptom"] = recovered_symptom

                symptom = merged_entities.get("symptom", "")
                if not symptom:
                    follow_up = "ä¸ºäº†ç»§ç»­åˆ†è¯Šï¼Œè¯·å…ˆå‘Šè¯‰æˆ‘å®å®çš„ä¸»è¦ç—‡çŠ¶ï¼ˆå¦‚å‘çƒ§ã€å’³å—½ã€è…¹æ³»ç­‰ï¼‰ã€‚"
                    conversation_service.append_message(conversation_id, request.user_id, "assistant", follow_up)
                    metadata_chunk = StreamChunk(
                        type="metadata",
                        metadata={
                            "intent": "slot_filling",
                            "need_follow_up": True,
                            "missing_slots": ["symptom"]
                        }
                    )
                    yield f"data: {metadata_chunk.model_dump_json()}\n\n"

                    chunk = StreamChunk(type="content", content=follow_up)
                    yield f"data: {chunk.model_dump_json()}\n\n"
                    yield "data: {\"type\": \"done\"}\n\n"
                    return

                danger_alert = triage_engine.check_danger_signals(merged_entities)
                if danger_alert:
                    conversation_service.append_message(conversation_id, request.user_id, "assistant", danger_alert)

                    metadata_chunk = StreamChunk(
                        type="metadata",
                        metadata={
                            "intent": "triage",
                            "origin_intent": "slot_filling",
                            "triage_level": "emergency",
                            "danger_signal": True
                        }
                    )
                    yield f"data: {metadata_chunk.model_dump_json()}\n\n"

                    chunk = StreamChunk(type="content", content=danger_alert)
                    yield f"data: {chunk.model_dump_json()}\n\n"
                    yield "data: {\"type\": \"done\"}\n\n"
                    return

                missing_slots = triage_engine.get_missing_slots(
                    symptom,
                    merged_entities,
                    profile_context=context
                )

                if missing_slots:
                    follow_up = triage_engine.generate_follow_up_question(symptom, missing_slots)
                    conversation_service.append_message(conversation_id, request.user_id, "assistant", follow_up)

                    slot_definitions = {}
                    for slot in missing_slots:
                        slot_definitions[slot] = {
                            "type": _get_slot_type(slot),
                            "label": _get_slot_label(slot),
                            "required": True,
                            "options": _get_slot_options(slot),
                            "min": _get_slot_min(slot),
                            "max": _get_slot_max(slot),
                            "step": _get_slot_step(slot)
                        }

                    metadata_chunk = StreamChunk(
                        type="metadata",
                        metadata={
                            "intent": "triage",
                            "origin_intent": "slot_filling",
                            "need_follow_up": True,
                            "missing_slots": slot_definitions
                        }
                    )
                    yield f"data: {metadata_chunk.model_dump_json()}\n\n"

                    chunk = StreamChunk(type="content", content=follow_up)
                    yield f"data: {chunk.model_dump_json()}\n\n"
                    yield "data: {\"type\": \"done\"}\n\n"
                    return

                decision = triage_engine.make_triage_decision(symptom, merged_entities)
                response_message = f"**{decision.reason}**\n\n{decision.action}"
                response_message = safety_filter.add_disclaimer(response_message)
                conversation_service.append_message(conversation_id, request.user_id, "assistant", response_message)

                metadata_chunk = StreamChunk(
                    type="metadata",
                    metadata={
                        "intent": "triage",
                        "origin_intent": "slot_filling",
                        "triage_level": decision.level,
                        "entities": merged_entities
                    }
                )
                yield f"data: {metadata_chunk.model_dump_json()}\n\n"

                asyncio.create_task(
                    profile_service.schedule_delayed_extraction(
                        user_id=request.user_id,
                        conversation_id=conversation_id,
                        delay_minutes=30
                    )
                )

                chunk_size = settings.STREAM_CHUNK_SIZE
                for i in range(0, len(response_message), chunk_size):
                    text_chunk = response_message[i:i + chunk_size]
                    chunk = StreamChunk(type="content", content=text_chunk)
                    yield f"data: {chunk.model_dump_json()}\n\n"

                yield "data: {\"type\": \"done\"}\n\n"
                return

            # 3. å¦‚æœæ˜¯åˆ†è¯Šæ„å›¾
            if intent_result.intent.type == "triage":
                conversation_service.append_message(conversation_id, request.user_id, "user", request.message)

                # æ£€æŸ¥å±é™©ä¿¡å·
                danger_alert = triage_engine.check_danger_signals(intent_result.entities)
                if danger_alert:
                    conversation_service.append_message(conversation_id, request.user_id, "assistant", danger_alert)

                    metadata_chunk = StreamChunk(
                        type="metadata",
                        metadata={
                            "intent": "triage",
                            "triage_level": "emergency",
                            "danger_signal": True
                        }
                    )
                    yield f"data: {metadata_chunk.model_dump_json()}\n\n"

                    chunk = StreamChunk(type="content", content=danger_alert)
                    yield f"data: {chunk.model_dump_json()}\n\n"
                    yield "data: {\"type\": \"done\"}\n\n"
                    return

                # æ£€æŸ¥ç¼ºå¤±æ§½ä½
                symptom = intent_result.entities.get("symptom", "")
                missing_slots = triage_engine.get_missing_slots(
                    symptom,
                    intent_result.entities,
                    profile_context=context
                )

                if missing_slots:
                    follow_up = triage_engine.generate_follow_up_question(symptom, missing_slots)
                    conversation_service.append_message(conversation_id, request.user_id, "assistant", follow_up)

                    slot_definitions = {}
                    for slot in missing_slots:
                        slot_definitions[slot] = {
                            "type": _get_slot_type(slot),
                            "label": _get_slot_label(slot),
                            "required": True,
                            "options": _get_slot_options(slot),
                            "min": _get_slot_min(slot),
                            "max": _get_slot_max(slot),
                            "step": _get_slot_step(slot)
                        }

                    metadata_chunk = StreamChunk(
                        type="metadata",
                        metadata={
                            "intent": "triage",
                            "need_follow_up": True,
                            "missing_slots": slot_definitions
                        }
                    )
                    yield f"data: {metadata_chunk.model_dump_json()}\n\n"

                    chunk = StreamChunk(type="content", content=follow_up)
                    yield f"data: {chunk.model_dump_json()}\n\n"
                    yield "data: {\"type\": \"done\"}\n\n"
                    return

                decision = triage_engine.make_triage_decision(symptom, intent_result.entities)
                response_message = f"**{decision.reason}**\n\n{decision.action}"
                response_message = safety_filter.add_disclaimer(response_message)
                conversation_service.append_message(conversation_id, request.user_id, "assistant", response_message)

                metadata_chunk = StreamChunk(
                    type="metadata",
                    metadata={
                        "intent": "triage",
                        "triage_level": decision.level,
                        "entities": intent_result.entities
                    }
                )
                yield f"data: {metadata_chunk.model_dump_json()}\n\n"

                asyncio.create_task(
                    profile_service.schedule_delayed_extraction(
                        user_id=request.user_id,
                        conversation_id=conversation_id,
                        delay_minutes=30
                    )
                )

                chunk_size = settings.STREAM_CHUNK_SIZE
                for i in range(0, len(response_message), chunk_size):
                    text_chunk = response_message[i:i + chunk_size]
                    chunk = StreamChunk(type="content", content=text_chunk)
                    yield f"data: {chunk.model_dump_json()}\n\n"

                yield "data: {\"type\": \"done\"}\n\n"
                return

            # 4. å…¶ä»–æ„å›¾ - ä½¿ç”¨RAGç”Ÿæˆå®Œæ•´ç­”æ¡ˆåå†æµå¼è¾“å‡º
            conversation_service.append_message(conversation_id, request.user_id, "user", request.message)

            # æ£€æµ‹æƒ…ç»ªå¹¶æ·»åŠ æƒ…ç»ªæ‰¿æ¥
            emotion_support = llm_service.detect_emotion(request.message)

            rag_result = await rag_service.generate_answer_with_sources(
                query=request.message,
                context=context
            )

            # æ·»åŠ æƒ…ç»ªæ‰¿æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if emotion_support:
                response_message = f"{emotion_support}\n\n{rag_result.answer}"
            else:
                response_message = rag_result.answer

            # æ·»åŠ å…è´£å£°æ˜
            response_message = safety_filter.add_disclaimer(response_message)

            # Send metadata
            # æ·»åŠ æ¥æºå…ƒæ•°æ®
            sources_metadata = rag_service.get_sources_metadata(rag_result.sources)
            metadata_chunk = StreamChunk(
                type="metadata",
                metadata={
                    "intent": intent_result.intent.type,
                    "has_source": rag_result.has_source,
                    "emotion_detected": emotion_support is not None,
                    "sources": sources_metadata
                }
            )
            yield f"data: {metadata_chunk.model_dump_json()}\n\n"

            asyncio.create_task(
                profile_service.schedule_delayed_extraction(
                    user_id=request.user_id,
                    conversation_id=conversation_id,
                    delay_minutes=30
                )
            )

            # æµå¼å®‰å…¨æ£€æŸ¥ - åœ¨è¾“å‡ºè¿‡ç¨‹ä¸­å®æ—¶æ£€æµ‹è¿ç¦è¯
            stream_filter = StreamSafetyFilter()
            full_output = ""
            chunk_size = settings.STREAM_CHUNK_SIZE
            for i in range(0, len(response_message), chunk_size):
                text_chunk = response_message[i:i + chunk_size]

                # æ£€æŸ¥å½“å‰å—æ˜¯å¦åŒ…å«è¿ç¦è¯
                safety_check = stream_filter.check_chunk(text_chunk)
                if safety_check.should_abort:
                    # å‘é€ä¸­æ­¢ä¿¡å·å’Œå®‰å…¨è­¦ç¤º
                    abort_chunk = StreamChunk(type="abort", content=safety_check.fallback_message)
                    yield f"data: {abort_chunk.model_dump_json()}\n\n"
                    yield "data: {\"type\": \"done\"}\n\n"
                    # è®°å½•å®‰å…¨æ‹¦æˆª
                    conversation_service.append_message(conversation_id, request.user_id, "assistant", safety_check.fallback_message)
                    return

                full_output += text_chunk
                chunk = StreamChunk(type="content", content=text_chunk)
                yield f"data: {chunk.model_dump_json()}\n\n"

            conversation_service.append_message(conversation_id, request.user_id, "assistant", full_output)
            yield "data: {\"type\": \"done\"}\n\n"

        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            error_chunk = StreamChunk(
                type="content",
                content="æŠ±æ­‰ï¼Œç³»ç»Ÿå‡ºç°å¼‚å¸¸ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            )
            yield f"data: {error_chunk.model_dump_json()}\n\n"
            yield "data: {\"type\": \"done\"}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


@router.get("/history/{conversation_id}")
async def get_conversation_history(conversation_id: str):
    """
    è·å–å¯¹è¯å†å²

    Args:
        conversation_id: å¯¹è¯ID

    Returns:
        dict: å¯¹è¯å†å²
    """
    try:
        messages = conversation_service.get_history(conversation_id)
        return {
            "code": 0,
            "data": {
                "conversation_id": conversation_id,
                "messages": messages
            }
        }
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯å†å²å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/source/{entry_id}")
async def get_source_snippet(entry_id: str):
    """
    è·å–çŸ¥è¯†åº“åŸæ–‡ç‰‡æ®µ

    Args:
        entry_id: çŸ¥è¯†åº“æ¡ç›®ID

    Returns:
        dict: åŸæ–‡ç‰‡æ®µ
    """
    try:
        entry = rag_service.get_entry_by_id(entry_id)
        if not entry:
            raise HTTPException(status_code=404, detail="æ¥æºæœªæ‰¾åˆ°")

        return {
            "code": 0,
            "data": {
                "id": entry.get("id"),
                "title": entry.get("title"),
                "source": entry.get("source"),
                "content": entry.get("content"),
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ¥æºå¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# ============ Helper Functions ============


def _recover_symptom_from_history(history: List[Dict[str, str]]) -> Optional[str]:
    """ä»å¯¹è¯å†å²ä¸­æ¢å¤æœ€è¿‘çš„ç—‡çŠ¶"""
    for item in reversed(history):
        if item.get("role") != "user":
            continue
        content = (item.get("content") or "").strip()
        if not content:
            continue
        result = llm_service._extract_intent_and_entities_fallback(content)
        symptom = result.entities.get("symptom")
        if symptom:
            return symptom
    return None

def _get_slot_type(slot_name: str) -> str:
    """Get slot input type"""
    slot_type_map = {
        "age_months": "number",
        "temperature": "number",
        "duration": "text",
        "frequency": "text",
        "mental_state": "select",
        "accompanying_symptoms": "multiselect",
        "fall_height": "select",
        "stool_character": "select",
        "cough_type": "select",
        "rash_location": "select",
        "rash_appearance": "select",
        "cry_pattern": "select",
    }
    return slot_type_map.get(slot_name, "text")


def _get_slot_label(slot_name: str) -> str:
    """Get slot display label"""
    slot_labels = {
        "age_months": "å®å®æœˆé¾„",
        "temperature": "ä½“æ¸©ï¼ˆÂ°Cï¼‰",
        "duration": "æŒç»­æ—¶é—´",
        "frequency": "é¢‘ç‡/æ¬¡æ•°",
        "mental_state": "ç²¾ç¥çŠ¶æ€",
        "accompanying_symptoms": "ä¼´éšç—‡çŠ¶",
        "fall_height": "å è½é«˜åº¦",
        "stool_character": "å¤§ä¾¿æ€§çŠ¶",
        "cough_type": "å’³å—½ç±»å‹",
        "rash_location": "çš®ç–¹ä½ç½®",
        "rash_appearance": "çš®ç–¹å¤–è§‚",
        "cry_pattern": "å“­é—¹æ¨¡å¼",
    }
    return slot_labels.get(slot_name, slot_name)


def _get_slot_options(slot_name: str) -> list:
    """Get slot options for select/multiselect types"""
    slot_options = {
        "mental_state": [
            {"value": "æ­£å¸¸", "label": "æ­£å¸¸ç©è€"},
            {"value": "ç²¾ç¥å·®", "label": "ç²¾ç¥å·®/è”«"},
            {"value": "å—œç¡", "label": "å—œç¡"},
            {"value": "çƒ¦èº", "label": "çƒ¦èºä¸å®‰"},
            {"value": "æ˜è¿·", "label": "æ˜è¿·"},
        ],
        "accompanying_symptoms": [
            {"value": "å’³å—½", "label": "å’³å—½"},
            {"value": "å‘•å", "label": "å‘•å"},
            {"value": "è…¹æ³»", "label": "è…¹æ³»"},
            {"value": "çš®ç–¹", "label": "çš®ç–¹"},
            {"value": "å‘¼å¸æ€¥ä¿ƒ", "label": "å‘¼å¸æ€¥ä¿ƒ"},
            {"value": "æŠ½æ", "label": "æŠ½æ"},
            {"value": "æ— ", "label": "æ— å…¶ä»–ç—‡çŠ¶"},
        ],
        "fall_height": [
            {"value": "åºŠ", "label": "åºŠï¼ˆ<50cmï¼‰"},
            {"value": "æ²™å‘", "label": "æ²™å‘ï¼ˆ<100cmï¼‰"},
            {"value": "æ¥¼æ¢¯", "label": "æ¥¼æ¢¯ï¼ˆ>100cmï¼‰"},
            {"value": "çª—æˆ·", "label": "çª—æˆ·/é˜³å°ï¼ˆ>200cmï¼‰"},
            {"value": "å…¶ä»–", "label": "å…¶ä»–"},
        ],
        "stool_character": [
            {"value": "æ°´æ ·", "label": "æ°´æ ·ä¾¿"},
            {"value": "ç³ŠçŠ¶", "label": "ç³ŠçŠ¶ä¾¿"},
            {"value": "é»æ¶²", "label": "é»æ¶²ä¾¿"},
            {"value": "è„“è¡€", "label": "è„“è¡€ä¾¿"},
            {"value": "è„‚è‚ª", "label": "è„‚è‚ªä¾¿ï¼ˆå¥¶ç“£ï¼‰"},
        ],
        "cough_type": [
            {"value": "å¹²å’³", "label": "å¹²å’³"},
            {"value": "æœ‰ç—°", "label": "æœ‰ç—°å’³"},
            {"value": "çŠ¬å æ ·", "label": "çŠ¬å æ ·å’³å—½"},
            {"value": "ç—‰æŒ›æ€§", "label": "ç—‰æŒ›æ€§å’³å—½"},
        ],
        "rash_location": [
            {"value": "é¢éƒ¨", "label": "é¢éƒ¨"},
            {"value": "èº¯å¹²", "label": "èº¯å¹²"},
            {"value": "å››è‚¢", "label": "å››è‚¢"},
            {"value": "å…¨èº«", "label": "å…¨èº«"},
            {"value": "å°¿å¸ƒåŒº", "label": "å°¿å¸ƒåŒº"},
        ],
        "rash_appearance": [
            {"value": "çº¢ç‚¹", "label": "çº¢ç‚¹/çº¢æ–‘"},
            {"value": "æ°´æ³¡", "label": "æ°´æ³¡"},
            {"value": "è„“åŒ…", "label": "è„“åŒ…"},
            {"value": "è„±çš®", "label": "è„±çš®"},
            {"value": "ç´«ç™œ", "label": "ç´«ç™œ"},
        ],
        "cry_pattern": [
            {"value": "é—´æ­‡æ€§", "label": "é—´æ­‡æ€§"},
            {"value": "æŒç»­æ€§", "label": "æŒç»­æ€§"},
            {"value": "å°–å«æ ·", "label": "å°–å«æ ·å“­"},
            {"value": "å‘»åŸ", "label": "å‘»åŸ"},
        ],
    }
    return slot_options.get(slot_name, [])


def _get_slot_min(slot_name: str) -> float:
    """Get minimum value for numeric slots"""
    slot_min = {
        "age_months": 0,
        "temperature": 35.0,
    }
    return slot_min.get(slot_name)


def _get_slot_max(slot_name: str) -> float:
    """Get maximum value for numeric slots"""
    slot_max = {
        "age_months": 216,  # 18 years
        "temperature": 42.0,
    }
    return slot_max.get(slot_name)


def _get_slot_step(slot_name: str) -> float:
    """Get step value for numeric slots"""
    slot_step = {
        "age_months": 1,
        "temperature": 0.1,
    }
    return slot_step.get(slot_name)


@router.get("/conversations/{user_id}")
async def get_user_conversations(user_id: str):
    """
    è·å–ç”¨æˆ·çš„æ‰€æœ‰å¯¹è¯

    Args:
        user_id: ç”¨æˆ·ID

    Returns:
        dict: å¯¹è¯åˆ—è¡¨
    """
    try:
        conversations = conversation_service.get_user_conversations(user_id)
        return {
            "code": 0,
            "data": {
                "conversations": conversations,
                "total": len(conversations)
            }
        }
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/conversations/{user_id}/{conversation_id}")
async def delete_conversation(user_id: str, conversation_id: str):
    """
    åˆ é™¤æŒ‡å®šå¯¹è¯

    Args:
        user_id: ç”¨æˆ·ID
        conversation_id: å¯¹è¯ID

    Returns:
        dict: åˆ é™¤ç»“æœ
    """
    try:
        success = conversation_service.delete_conversation(conversation_id, user_id)
        if not success:
            raise HTTPException(status_code=404, detail="å¯¹è¯æœªæ‰¾åˆ°")

        return {
            "code": 0,
            "data": {
                "conversation_id": conversation_id,
                "deleted": True
            },
            "message": "å¯¹è¯å·²åˆ é™¤"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤å¯¹è¯å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/conversations/{user_id}")
async def create_conversation(user_id: str):
    """
    åˆ›å»ºæ–°å¯¹è¯

    Args:
        user_id: ç”¨æˆ·ID

    Returns:
        dict: æ–°å¯¹è¯ä¿¡æ¯
    """
    try:
        conversation_id = f"conv_{uuid.uuid4().hex[:12]}"
        conversation = conversation_service.create_conversation(conversation_id, user_id)

        return {
            "code": 0,
            "data": conversation
        }
    except Exception as e:
        logger.error(f"åˆ›å»ºå¯¹è¯å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
