"""
RAGæœåŠ¡ - çŸ¥è¯†åº“æ£€ç´¢ä¸å†…å®¹æº¯æº
"""
import json
import os
import math
import re
import time
from collections import Counter
from typing import List, Dict, Any, Optional
from loguru import logger
from openai import OpenAI

from app.config import settings
from app.models.user import KnowledgeSource, RAGResult


class RAGService:
    """RAGæ£€ç´¢æœåŠ¡"""

    def __init__(self):
        """åˆå§‹åŒ–"""
        self.client = OpenAI(
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL
        )
        self.chat_model = settings.DEEPSEEK_MODEL
        self.embedding_model = settings.EMBEDDING_MODEL
        self.knowledge_base = self._load_knowledge_base()
        self.embeddings_cache = {}
        self._api_key_configured = bool(settings.DEEPSEEK_API_KEY)
        self._remote_cooldown_until: float = 0.0
        self._doc_token_counts: List[Counter] = []
        self._build_local_index()

    @property
    def remote_available(self) -> bool:
        if not self._api_key_configured:
            return False
        return time.time() >= self._remote_cooldown_until

    @remote_available.setter
    def remote_available(self, value: bool):
        if not value:
            self._remote_cooldown_until = time.time() + 60  # 60ç§’å†·å´
        else:
            self._remote_cooldown_until = 0.0

    def _load_knowledge_base(self) -> List[Dict[str, Any]]:
        """åŠ è½½çŸ¥è¯†åº“"""
        knowledge_base = []
        kb_path = settings.KNOWLEDGE_BASE_PATH

        try:
            # éå†çŸ¥è¯†åº“ç›®å½•ä¸‹çš„æ‰€æœ‰JSONæ–‡ä»¶
            for filename in os.listdir(kb_path):
                if filename.endswith('.json'):
                    filepath = os.path.join(kb_path, filename)
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        # å±•å¼€entries
                        for entry in data.get('entries', []):
                            entry['topic'] = data.get('topic')
                            entry['category'] = data.get('category')
                            knowledge_base.append(entry)

            logger.info(f"åŠ è½½çŸ¥è¯†åº“å®Œæˆï¼Œå…± {len(knowledge_base)} æ¡è®°å½•")
            return knowledge_base

        except Exception as e:
            logger.error(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}", exc_info=True)
            return []

    async def get_embedding(self, text: str) -> Optional[List[float]]:
        """
        è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º

        Args:
            text: æ–‡æœ¬

        Returns:
            Optional[List[float]]: å‘é‡
        """
        # æ£€æŸ¥ç¼“å­˜
        if text in self.embeddings_cache:
            return self.embeddings_cache[text]

        try:
            response = self.client.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            embedding = response.data[0].embedding
            self.embeddings_cache[text] = embedding
            return embedding
        except Exception as e:
            logger.error(f"è·å–embeddingå¼‚å¸¸: {e}", exc_info=True)
            return None

    async def retrieve(
        self,
        query: str,
        top_k: int = 3,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[KnowledgeSource]:
        """
        æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼ˆæ··åˆæ£€ç´¢ + é‡æ’åºï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„æ–‡æ¡£æ•°
            filters: è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚age_range, categoryç­‰ï¼‰
            
        Returns:
            List[KnowledgeSource]: æ£€ç´¢ç»“æœ
        """
        if not self.knowledge_base:
            logger.warning("çŸ¥è¯†åº“ä¸ºç©º")
            return []

        try:
            # 1. æ··åˆæ£€ç´¢å¬å› (Recall)
            candidates = await self._hybrid_search(query, top_k=50, filters=filters)
            
            # 2. é‡æ’åº (Rerank)
            results = await self._rerank(query, candidates, top_k=top_k)
            
            logger.info(f"æ£€ç´¢å®Œæˆ: å¬å›{len(candidates)}æ¡ -> é‡æ’åºé€‰å‡º{len(results)}æ¡")
            return results

        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            return []

    async def _hybrid_search(
        self, 
        query: str, 
        top_k: int = 50, 
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥ï¼šè¯­ä¹‰æ£€ç´¢ (70%) + å…³é”®è¯æ£€ç´¢ (30%)
        """
        # 1. å‘é‡æ£€ç´¢ (Semantic Search)
        vector_candidates = []
        if self.remote_available and self.embedding_model != "local":
            query_embedding = await self.get_embedding(query)
            if query_embedding:
                for entry in self.knowledge_base:
                    if filters and not self._match_filters(entry, filters):
                        continue
                        
                    doc_text = f"{entry.get('title', '')} {entry.get('content', '')}"
                    # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥ç¼“å­˜doc_embeddingï¼Œç®€åŒ–èµ·è§å‡è®¾å·²ç¼“å­˜æˆ–æŒ‰éœ€è·å–
                    # å®é™…ç”Ÿäº§ä¸­åº”ä½¿ç”¨å‘é‡æ•°æ®åº“
                    doc_embedding = self.embeddings_cache.get(doc_text)
                    if not doc_embedding:
                        # é¿å…å®æ—¶å¤§é‡è°ƒç”¨Embedding APIï¼Œè¿™é‡Œä»…ä½œæ¼”ç¤º
                        # å®é™…åº”é¢„å…ˆè®¡ç®—å¥½æ‰€æœ‰æ–‡æ¡£Embedding
                        continue
                        
                    similarity = self._cosine_similarity(query_embedding, doc_embedding)
                    vector_candidates.append({
                        "entry": entry,
                        "vector_score": float(similarity),
                        "keyword_score": 0.0
                    })

        # 2. å…³é”®è¯æ£€ç´¢ (Keyword Search - BM25-like)
        # å¦‚æœå‘é‡æ£€ç´¢ä¸å¯ç”¨æˆ–ä¸ºäº†å¢å¼ºæ•ˆæœï¼Œè®¡ç®—å…³é”®è¯åˆ†æ•°
        keyword_candidates = []
        query_counter = self._text_to_counter(query)

        # åŒä¹‰è¯æ˜ å°„ï¼šå£è¯­åŒ–è¡¨è¾¾ â†’ æ ‡å‡†æœ¯è¯­
        synonym_mapping = {
            "æ‹‰è‚šå­": "è…¹æ³»", "æ‹‰ç¨€": "è…¹æ³»",
            "å‘çƒ§": "å‘çƒ­", "é«˜çƒ§": "å‘çƒ­",
            "å": "å‘•å", "åå¥¶": "å‘•å",
            "å’³": "å’³å—½",
            "èµ·ç–¹å­": "çš®ç–¹", "æ¹¿ç–¹": "çš®ç–¹",
            "æ‘”ä¼¤": "æ‘”å€’", "è·Œå€’": "æ‘”å€’", "è·Œè½": "æ‘”å€’",
            "ä¾¿ç§˜": "å¤§ä¾¿å›°éš¾"
        }

        # å¯¹æŸ¥è¯¢è¿›è¡ŒåŒä¹‰è¯æ‰©å±•
        expanded_query_tokens = set(query_counter.keys())
        for token in list(query_counter.keys()):
            if token in synonym_mapping:
                expanded_query_tokens.add(synonym_mapping[token])

        # ä¹Ÿéœ€è¦åå‘æ‰©å±•ï¼šå¦‚æœæ–‡æ¡£æœ‰æ ‡å‡†æœ¯è¯­ï¼ŒæŸ¥è¯¢æœ‰å£è¯­è¡¨è¾¾ï¼Œåº”è¯¥åŒ¹é…
        reverse_synonym_mapping = {v: k for k, v in synonym_mapping.items()}
        for token in list(query_counter.keys()):
            if token in reverse_synonym_mapping:
                expanded_query_tokens.add(reverse_synonym_mapping[token])

        for idx, entry in enumerate(self.knowledge_base):
            if filters and not self._match_filters(entry, filters):
                continue

            # ä½¿ç”¨ç®€å•çš„è¯é¢‘é‡åˆåº¦ä½œä¸ºå…³é”®è¯åˆ†æ•°
            keyword_score = self._cosine_similarity_counts(query_counter, self._doc_token_counts[idx])

            # æ ‡é¢˜åŒ¹é…åŠ æƒ - åŒå‘æ£€æŸ¥
            title = entry.get("title", "")
            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åœ¨æŸ¥è¯¢ä¸­
            if title and title in query:
                keyword_score += 0.5
            # æ£€æŸ¥æŸ¥è¯¢å…³é”®è¯ï¼ˆå«åŒä¹‰è¯ï¼‰æ˜¯å¦åœ¨æ ‡é¢˜ä¸­ - ä½¿ç”¨å­å­—ç¬¦ä¸²åŒ¹é…
            elif title:
                # å¯¹äºä¸­æ–‡è¯æ±‡ï¼Œä½¿ç”¨å­å­—ç¬¦ä¸²åŒ¹é…æ›´å¯é 
                for query_token in expanded_query_tokens:
                    if len(query_token) > 1 and query_token in title:
                        keyword_score += 0.4
                        break

            # æ ‡ç­¾åŒ¹é…åŠ æƒ - åŒæ ·ä½¿ç”¨å­å­—ç¬¦ä¸²åŒ¹é…
            tags = entry.get("tags", [])
            for tag in tags:
                for query_token in expanded_query_tokens:
                    if len(query_token) > 1 and query_token in tag:
                        keyword_score += 0.2
                        break

            keyword_candidates.append({
                "entry": entry,
                "keyword_score": keyword_score
            })
            
        # 3. èåˆåˆ†æ•° (Fusion)
        # ä½¿ç”¨ç®€å•çš„åŠ æƒèåˆ: 0.7 * Vector + 0.3 * Keyword
        # éœ€å¤„ç† vector_candidates å’Œ keyword_candidates çš„åˆå¹¶
        
        # å»ºç«‹ entry_id -> candidate æ˜ å°„
        merged = {}
        
        # å¤„ç†å‘é‡ç»“æœ
        for item in vector_candidates:
            eid = item["entry"].get("id")
            merged[eid] = item
            
        # å¤„ç†å…³é”®è¯ç»“æœ
        for item in keyword_candidates:
            eid = item["entry"].get("id")
            if eid in merged:
                merged[eid]["keyword_score"] = item["keyword_score"]
            else:
                merged[eid] = {
                    "entry": item["entry"],
                    "vector_score": 0.0, # æœªå‘½ä¸­å‘é‡æ£€ç´¢
                    "keyword_score": item["keyword_score"]
                }
                
        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        final_candidates = []
        for item in merged.values():
            # å½’ä¸€åŒ–åˆ†æ•° (å‡è®¾åˆ†æ•°éƒ½åœ¨ 0-1 ä¹‹é—´)
            v_score = item.get("vector_score", 0.0)
            k_score = item.get("keyword_score", 0.0)
            
            # æ··åˆæƒé‡
            if self.remote_available:
                final_score = 0.7 * v_score + 0.3 * k_score
            else:
                final_score = k_score # ä»…ä½¿ç”¨å…³é”®è¯
                
            item["score"] = final_score
            final_candidates.append(item)
            
        # æ’åºå¹¶æˆªå–
        final_candidates.sort(key=lambda x: x["score"], reverse=True)
        return final_candidates[:top_k]

    async def _rerank(
        self, 
        query: str, 
        candidates: List[Dict[str, Any]], 
        top_k: int = 3
    ) -> List[KnowledgeSource]:
        """
        é‡æ’åº (Reranking)
        æ¨¡æ‹Ÿ Cross-Encoder çš„æ•ˆæœï¼Œå¯¹å¬å›ç»“æœè¿›è¡Œç²¾ç»†æ‰“åˆ†
        """
        # ç”±äºç¯å¢ƒé™åˆ¶æ— æ³•è¿è¡Œ BGE-Rerankerï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™æ¨¡æ‹Ÿ
        reranked = []
        
        for item in candidates:
            entry = item["entry"]
            base_score = item["score"]
            rerank_score = base_score
            
            content = entry.get("content", "")
            title = entry.get("title", "")
            
            # è§„åˆ™1: ç²¾ç¡®çŸ­è¯­åŒ¹é…å¥–åŠ±
            if query in content:
                rerank_score += 0.2

            # è§„åˆ™2: å…³é”®åŒ»å­¦å®ä½“åŒ¹é… (æ¨¡æ‹Ÿ)
            # æ¯”å¦‚æŸ¥è¯¢åŒ…å«"æ³°è¯ºæ—"ï¼Œæ–‡æ¡£æ ‡é¢˜ä¹ŸåŒ…å«
            if "æ³°è¯ºæ—" in query and "æ³°è¯ºæ—" in title:
                rerank_score += 0.3
            if "ç¾æ—" in query and "ç¾æ—" in title:
                rerank_score += 0.3

            # è§„åˆ™3: åŒä¹‰è¯/å£è¯­åŒ–è¡¨è¾¾åŒ¹é…å¥–åŠ±
            # å¦‚æœæŸ¥è¯¢åŒ…å« "æ‹‰è‚šå­" è€Œæ–‡æ¡£åŒ…å« "è…¹æ³»"
            diarrhea_keywords = ["æ‹‰è‚šå­", "æ‹‰ç¨€", "è…¹æ³»"]
            if any(kw in query for kw in diarrhea_keywords) and "è…¹æ³»" in title:
                rerank_score += 0.2

            fever_keywords = ["å‘çƒ§", "å‘çƒ­", "é«˜çƒ§"]
            if any(kw in query for kw in fever_keywords) and any(kw in title for kw in fever_keywords):
                rerank_score += 0.2

            cough_keywords = ["å’³å—½", "å’³"]
            if any(kw in query for kw in cough_keywords) and "å’³å—½" in title:
                rerank_score += 0.2

            vomit_keywords = ["å‘•å", "å", "åå¥¶"]
            if any(kw in query for kw in vomit_keywords) and "å‘•å" in title:
                rerank_score += 0.2

            rash_keywords = ["çš®ç–¹", "ç–¹å­", "æ¹¿ç–¹"]
            if any(kw in query for kw in rash_keywords) and "çš®ç–¹" in title:
                rerank_score += 0.2
                
            # è§„åˆ™3: è´Ÿå‘æƒ©ç½š (å¦‚æœæŸ¥è¯¢æ˜¯"ä¸å‘çƒ§"ä½†æ–‡æ¡£å…¨æ˜¯"å‘çƒ§")
            # (ç•¥ï¼Œè¿‡äºå¤æ‚)
            
            # é˜ˆå€¼è¿‡æ»¤
            if rerank_score < settings.SIMILARITY_THRESHOLD and not self.remote_available:
                 # æœ¬åœ°æ¨¡å¼ç¨å¾®æ”¾å®½
                 if rerank_score < 0.1: continue
            elif rerank_score < settings.SIMILARITY_THRESHOLD:
                 continue

            reranked.append(KnowledgeSource(
                content=content,
                source=entry.get('source', 'æœªçŸ¥æ¥æº'),
                score=rerank_score,
                metadata={
                    'id': entry.get('id'),
                    'title': title,
                    'topic': entry.get('topic'),
                    'category': entry.get('category'),
                    'tags': entry.get('tags', []),
                    'age_range': entry.get('age_range'),
                    'alert_level': entry.get('alert_level'),
                    'retrieval_info': {
                        'vector_score': item.get('vector_score', 0),
                        'keyword_score': item.get('keyword_score', 0)
                    }
                }
            ))
            
        # å†æ¬¡æ’åº
        reranked.sort(key=lambda x: x.score, reverse=True)
        return reranked[:top_k]

    def _match_filters(self, entry: Dict[str, Any], filters: Dict[str, Any]) -> bool:
        """æ£€æŸ¥entryæ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶"""
        for key, value in filters.items():
            if key == 'age_months':
                # æ£€æŸ¥å¹´é¾„èŒƒå›´
                age_range = entry.get('age_range', '')
                if not self._in_age_range(value, age_range):
                    return False
            elif key in entry:
                if entry[key] != value:
                    return False
        return True

    def _in_age_range(self, age_months: int, age_range_str: str) -> bool:
        """æ£€æŸ¥å¹´é¾„æ˜¯å¦åœ¨èŒƒå›´å†…"""
        if not age_range_str:
            return True

        try:
            # è§£æ "0-36ä¸ªæœˆ" æ ¼å¼
            if '-' in age_range_str and 'ä¸ªæœˆ' in age_range_str:
                parts = age_range_str.replace('ä¸ªæœˆ', '').split('-')
                min_age = int(parts[0])
                max_age = int(parts[1])
                return min_age <= age_months <= max_age
        except (ValueError, IndexError):
            pass

        return True

    async def generate_answer_with_sources(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None
    ) -> RAGResult:
        """
        åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ

        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: ä¸Šä¸‹æ–‡ï¼ˆç”¨æˆ·æ¡£æ¡ˆç­‰ï¼‰

        Returns:
            RAGResult: ç­”æ¡ˆå’Œæ¥æº
        """
        # 1. æ£€ç´¢ç›¸å…³çŸ¥è¯†
        filters = {}
        if context and context.get('baby_info', {}).get('age_months'):
            filters['age_months'] = context['baby_info']['age_months']

        sources = await self.retrieve(query, top_k=3, filters=filters)

        # 2. å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³çŸ¥è¯†ï¼Œè¿”å›æ‹’ç­”
        if not sources:
            return RAGResult(
                answer="æŠ±æ­‰ï¼Œæˆ‘çš„æƒå¨çŸ¥è¯†åº“ä¸­æš‚æ— å…³äºæ­¤é—®é¢˜çš„è®°å½•ã€‚å»ºè®®æ‚¨å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚",
                sources=[],
                has_source=False
            )

        # 3. æ„å»ºpromptï¼Œè®©LLMåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
        prompt = self._build_rag_prompt(query, sources, context)

        # 4. ç”Ÿæˆç­”æ¡ˆï¼ˆéæµå¼ï¼‰
        try:
            if not self.remote_available:
                answer = self._build_fallback_answer(sources)
                answer_with_citations = self.format_with_citations(answer, sources)
                return RAGResult(
                    answer=answer_with_citations,
                    sources=sources,
                    has_source=True
                )

            response = self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "system", "content": self._get_rag_system_prompt()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
            )

            answer = response.choices[0].message.content

            # 5. æ·»åŠ æº¯æºè§’æ ‡
            answer_with_citations = self.format_with_citations(answer, sources)

            return RAGResult(
                answer=answer_with_citations,
                sources=sources,
                has_source=True
            )
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆå¼‚å¸¸: {e}", exc_info=True)
            self.remote_available = False
            answer = self._build_fallback_answer(sources)
            answer_with_citations = self.format_with_citations(answer, sources)
            return RAGResult(
                answer=answer_with_citations,
                sources=sources,
                has_source=True
            )

    def _build_fallback_answer(self, sources: List[KnowledgeSource]) -> str:
        """æœ¬åœ°å…œåº•å›ç­”ï¼ˆæ— éœ€LLMï¼‰"""
        top = sources[0]
        entry_id = top.metadata.get("id", "unknown")
        title = top.metadata.get("title", "å‚è€ƒå»ºè®®")
        content = top.content

        return (
            f"**æ ¸å¿ƒç»“è®º**ï¼š{title}ã€æ¥æº:{entry_id}ã€‘\n\n"
            f"**æ“ä½œå»ºè®®**ï¼š\n{content}ã€æ¥æº:{entry_id}ã€‘\n\n"
            "**æ³¨æ„äº‹é¡¹**ï¼š\n"
            "- è¯·ç»“åˆå®å®å®é™…æƒ…å†µè§‚å¯Ÿå˜åŒ–\n"
            "- å¦‚æœ‰ç–‘é—®è¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ\n\n"
            "**âš ï¸ ç«‹å³å°±åŒ»ä¿¡å·**ï¼š\n"
            "å¦‚æœå‡ºç°ä»¥ä¸‹æƒ…å†µï¼Œè¯·ç«‹åˆ»å‰å¾€åŒ»é™¢ï¼š\n"
            "- ç—‡çŠ¶æŒç»­åŠ é‡æˆ–å‡ºç°æ–°çš„å¼‚å¸¸ç—‡çŠ¶\n"
            "- å®å®ç²¾ç¥çŠ¶æ€æ˜æ˜¾å˜å·®\n"
            "- å‡ºç°å‘¼å¸å›°éš¾ã€æŒç»­é«˜çƒ­ç­‰å±é™©ä¿¡å·\n\n"
            "**æ‚¨å¯èƒ½è¿˜æƒ³äº†è§£**ï¼š\n"
            "- æœ‰å“ªäº›éœ€è¦ç‰¹åˆ«æ³¨æ„çš„åœ°æ–¹ï¼Ÿ\n"
            "- ä»€ä¹ˆæƒ…å†µéœ€è¦å°±åŒ»ï¼Ÿ\n"
            "- å¦‚ä½•è§‚å¯Ÿå®å®çš„æ¢å¤æƒ…å†µï¼Ÿ"
        )
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
        dot = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        if norm1 == 0.0 or norm2 == 0.0:
            return 0.0
        return dot / (norm1 * norm2)

    def _build_local_index(self) -> None:
        """æ„å»ºæœ¬åœ°æ£€ç´¢ç´¢å¼•"""
        self._doc_token_counts = []
        for entry in self.knowledge_base:
            doc_text = f"{entry.get('title', '')} {entry.get('content', '')}"
            self._doc_token_counts.append(self._text_to_counter(doc_text))

    def _retrieve_local(
        self,
        query: str,
        top_k: int = 3,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[KnowledgeSource]:
        """æœ¬åœ°æ£€ç´¢ï¼ˆä¸ä¾èµ–å¤–éƒ¨embeddingï¼‰"""
        query_counter = self._text_to_counter(query)
        candidates = []
        for idx, entry in enumerate(self.knowledge_base):
            if filters and not self._match_filters(entry, filters):
                continue
            similarity = self._cosine_similarity_counts(query_counter, self._doc_token_counts[idx])
            title = entry.get("title", "")
            tags = entry.get("tags", [])
            if title and title in query:
                similarity = max(similarity, 0.8)
            if tags and any(tag in query for tag in tags):
                similarity = max(similarity, 0.6)
            candidates.append({
                "entry": entry,
                "similarity": float(similarity)
            })

        candidates.sort(key=lambda x: x["similarity"], reverse=True)
        top_candidates = candidates[:top_k]

        results = []
        local_threshold = 0.2
        for candidate in top_candidates:
            if candidate["similarity"] >= local_threshold:
                entry = candidate["entry"]
                results.append(KnowledgeSource(
                    content=entry.get("content", ""),
                    source=entry.get("source", "æœªçŸ¥æ¥æº"),
                    score=candidate["similarity"],
                    metadata={
                        "id": entry.get("id"),
                        "title": entry.get("title"),
                        "topic": entry.get("topic"),
                        "category": entry.get("category"),
                        "tags": entry.get("tags", []),
                        "age_range": entry.get("age_range"),
                        "alert_level": entry.get("alert_level")
                    }
                ))

        return results

    def _text_to_counter(self, text: str) -> Counter:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯é¢‘è®¡æ•°å™¨ï¼Œä¼˜å…ˆåŒ¹é…å¸¸è§åŒ»å­¦è¯æ±‡"""
        text_lower = text.lower()

        # å¸¸è§åŒ»å­¦è¯æ±‡åˆ—è¡¨ï¼ˆä¼˜å…ˆåŒ¹é…é•¿è¯ï¼‰
        medical_terms = [
            # ç—‡çŠ¶
            "æ‹‰è‚šå­", "è…¹æ³»", "å‘çƒ§", "å‘çƒ­", "å’³å—½", "å‘•å", "çš®ç–¹", "æ¹¿ç–¹",
            "æƒŠå¥", "æŠ½æ", "å‘¼å¸å›°éš¾", "æ˜è¿·", "ä¾¿ç§˜", "æ‘”å€’", "è·Œè½", "æ‘”ä¼¤",
            "è„±æ°´", "è¡¥æ¶²", "å—œç¡", "ç²¾ç¥èé¡",
            # é€šç”¨
            "å®å®", "å©´å„¿", "å¹¼å„¿", "å„¿ç«¥",
            # æ—¶é—´
            "å°æ—¶", "åˆ†é’Ÿ", "å¤©", "å‘¨", "æœˆ", "å¹´"
        ]

        tokens = []
        remaining = text_lower

        # å…ˆåŒ¹é…åŒ»å­¦è¯æ±‡
        for term in sorted(medical_terms, key=len, reverse=True):
            while term in remaining:
                tokens.append(term)
                # æ›¿æ¢å·²åŒ¹é…çš„éƒ¨åˆ†ä¸ºç©ºæ ¼ï¼Œé¿å…é‡å¤åŒ¹é…
                remaining = remaining.replace(term, " ", 1)

        # å¯¹å‰©ä½™æ–‡æœ¬æŒ‰å•å­—åˆ†è¯
        for char in remaining:
            if re.match(r"[a-zA-Z0-9]", char):
                tokens.append(char)
            elif re.match(r"[\u4e00-\u9fff]", char):
                tokens.append(char)

        return Counter(tokens)

    def _cosine_similarity_counts(self, c1: Counter, c2: Counter) -> float:
        if not c1 or not c2:
            return 0.0
        common = set(c1.keys()) & set(c2.keys())
        dot = sum(c1[token] * c2[token] for token in common)
        norm1 = math.sqrt(sum(v * v for v in c1.values()))
        norm2 = math.sqrt(sum(v * v for v in c2.values()))
        if norm1 == 0.0 or norm2 == 0.0:
            return 0.0
        return dot / (norm1 * norm2)

    def _build_rag_prompt(
        self,
        query: str,
        sources: List[KnowledgeSource],
        context: Optional[Dict[str, Any]]
    ) -> str:
        """æ„å»ºRAGæç¤ºè¯"""
        prompt = f"å®¶é•¿çš„é—®é¢˜ï¼š{query}\n\n"

        if context and context.get('baby_info'):
            baby_info = context['baby_info']
            prompt += "å®å®ä¿¡æ¯ï¼š\n"
            if baby_info.get('age_months'):
                prompt += f"- æœˆé¾„ï¼š{baby_info['age_months']}ä¸ªæœˆ\n"
            if baby_info.get('weight_kg'):
                prompt += f"- ä½“é‡ï¼š{baby_info['weight_kg']}kg\n"
            prompt += "\n"

        prompt += "ä»¥ä¸‹æ˜¯ä»æƒå¨åŒ»å­¦çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š\n\n"
        for i, source in enumerate(sources, 1):
            prompt += f"--- æ–‡æ¡£{i} ---\n"
            prompt += f"{source.content}\n\n"

        prompt += "è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹ï¼Œç”¨æ¸©æš–æ˜“æ‡‚çš„è¯­è¨€å›ç­”å®¶é•¿çš„é—®é¢˜ã€‚\n"
        prompt += "æ³¨æ„ï¼šä¸è¦åœ¨å›ç­”æ­£æ–‡ä¸­æ’å…¥æ¥æºæ ‡è®°æˆ–å¼•ç”¨ç¼–å·ã€‚\n"

        return prompt

    def _get_rag_system_prompt(self) -> str:
        """è·å–RAGç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ã€Œå°å„¿å®‰ã€ï¼Œä¸€ä½æ¸©æš–ã€ä¸“ä¸šçš„å„¿ç§‘å¥åº·é¡¾é—®ã€‚ä½ çš„ç”¨æˆ·æ˜¯ç„¦è™‘çš„æ–°æ‰‹çˆ¸å¦ˆï¼Œè¯·ç”¨æœ‹å‹èˆ¬çš„å£å»å’Œä»–ä»¬äº¤æµã€‚

## å›ç­”åŸåˆ™
1. ä¼˜å…ˆå¼•ç”¨çŸ¥è¯†åº“æ–‡æ¡£ä¸­çš„å†…å®¹ä½œä¸ºæ ¸å¿ƒä¾æ®
2. å½“æ–‡æ¡£æä¾›äº†éƒ¨åˆ†ä¿¡æ¯ä½†ä¸å¤Ÿå®Œæ•´æ—¶ï¼Œä½ å¯ä»¥åŸºäºå„¿ç§‘å¸¸è¯†è¿›è¡Œåˆç†è¡¥å……ï¼ˆå¦‚åŸºç¡€æŠ¤ç†å»ºè®®ï¼šå¤šä¼‘æ¯ã€æ³¨æ„è§‚å¯Ÿã€ä¿æŒé€šé£ç­‰ï¼‰ï¼Œä½†éœ€æ ‡æ³¨"ä¸€èˆ¬å»ºè®®"
3. å½“æ–‡æ¡£å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶ï¼Œå¦è¯šå‘ŠçŸ¥ï¼Œå¹¶ç»™å‡ºå°±åŒ»å»ºè®®ï¼Œè€Œä¸æ˜¯ç®€å•æ‹’ç»
4. ç»ä¸ç¼–é€ å…·ä½“æ•°æ®ï¼ˆå¦‚è¯ç‰©å‰‚é‡ã€ä½“æ¸©é˜ˆå€¼ï¼‰ï¼Œæ•°æ®å¿…é¡»æ¥è‡ªæ–‡æ¡£

## è¯­æ°”è¦æ±‚
- åƒä¸€ä½æœ‰ç»éªŒçš„å„¿ç§‘æŠ¤å£«åœ¨å’Œå®¶é•¿èŠå¤©ï¼Œæ¸©æš–ä½†ä¸å•°å—¦
- ç”¨"å®å®""æ‚¨"ç­‰äº²åˆ‡ç§°å‘¼
- é¿å…å­¦æœ¯è®ºæ–‡å¼çš„é•¿å¥ï¼Œå¤šç”¨çŸ­å¥å’Œå£è¯­åŒ–è¡¨è¾¾
- é€‚å½“ä½¿ç”¨ emoji å¢åŠ äº²å’ŒåŠ›ï¼ˆä¸è¶…è¿‡3ä¸ªï¼‰

## è¾“å‡ºæ ¼å¼ï¼ˆä½¿ç”¨æ ‡å‡† Markdownï¼Œç¡®ä¿æ’ç‰ˆæ¸…æ™°ï¼‰

### ğŸ“‹ å¿…é¡»éµå¾ªçš„æ’ç‰ˆè§„åˆ™

**1. æ®µè½ä¸æ¢è¡Œ**
- æ¯ä¸ªæ®µè½ä¹‹é—´å¿…é¡»ç©ºä¸€è¡Œï¼ˆä½¿ç”¨ä¸¤ä¸ªæ¢è¡Œç¬¦ï¼‰
- æ ‡é¢˜å‰åå¿…é¡»å„ç©ºä¸€è¡Œ
- ç»ä¸èƒ½æŠŠå¤šä¸ªæ®µè½æŒ¤åœ¨ä¸€èµ·

**2. åˆ—è¡¨æ ¼å¼ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰**
- æ•°å­—åˆ—è¡¨å¿…é¡»ä½¿ç”¨ Markdown æ ‡å‡†è¯­æ³•ï¼š`1. ` `2. ` `3. `ï¼ˆæ³¨æ„ç‚¹å·åé¢æœ‰ç©ºæ ¼ï¼‰
- æ¯ä¸ªåˆ—è¡¨é¡¹å¿…é¡»ç‹¬å ä¸€è¡Œï¼Œä¸¥ç¦å¤šä¸ªé¡¹å†™åœ¨åŒä¸€è¡Œ
- åˆ—è¡¨é¡¹ä¹‹é—´ä¸è¦ç©ºè¡Œï¼Œä½†åˆ—è¡¨æ•´ä½“å‰åè¦ç©ºä¸€è¡Œ

**3. æ ‡é¢˜å±‚çº§**
- ä½¿ç”¨ `###` ä½œä¸ºæ¿å—æ ‡é¢˜ï¼ˆå¦‚ `### ğŸ’¡ æ ¸å¿ƒç»“è®º`ï¼‰
- æ ‡é¢˜æ–‡å­—è¦ç®€æ´æœ‰åŠ›ï¼ŒåŠ ä¸Š Emoji å¢å¼ºè¯†åˆ«

**4. å¼ºè°ƒæ ¼å¼**
- å…³é”®ä¿¡æ¯ä½¿ç”¨ `**åŠ ç²—**`ï¼ˆå¦‚ `**ä½“æ¸©è¶…è¿‡39â„ƒ**`ï¼‰
- è¯ç‰©åç§°ã€ç—‡çŠ¶è¯ã€é‡è¦æ•°æ®å¿…é¡»åŠ ç²—
- ä¸è¦ä½¿ç”¨ `_æ–œä½“_` æˆ– `~~åˆ é™¤çº¿~~`

**5. ç´§æ€¥ä¿¡æ¯çªå‡º**
- å°±åŒ»è­¦ç¤ºä½¿ç”¨ `> ğŸš¨` å¼•ç”¨å—æ ¼å¼
- æˆ–è€…å•ç‹¬ä¸€è¡Œä»¥ `âš ï¸` å¼€å¤´

### ğŸ“ è¾“å‡ºæ¨¡æ¿ï¼ˆä¸¥æ ¼æŒ‰æ­¤ç»“æ„ï¼‰

### ğŸ’¡ æ ¸å¿ƒç»“è®º

[ä¸€å¥è¯æ€»ç»“ï¼Œ15å­—ä»¥å†…]


### ğŸ©º æŠ¤ç†å»ºè®®

1. [ç¬¬ä¸€ä¸ªå…·ä½“æ­¥éª¤ï¼Œä¸è¶…è¿‡20å­—]
2. [ç¬¬äºŒä¸ªå…·ä½“æ­¥éª¤]
3. [ç¬¬ä¸‰ä¸ªå…·ä½“æ­¥éª¤]


### âš ï¸ éœ€è¦æ³¨æ„

- [æ³¨æ„ç‚¹1ï¼Œç”¨ - å¼€å¤´]
- [æ³¨æ„ç‚¹2]


> ğŸš¨ **è¿™äº›æƒ…å†µè¯·ç«‹å³å°±åŒ»**
>
> - [å°±åŒ»ä¿¡å·1]
> - [å°±åŒ»ä¿¡å·2]
> - [å°±åŒ»ä¿¡å·3]


### ğŸ’­ æ‚¨å¯èƒ½è¿˜æƒ³äº†è§£

- [å¼•å¯¼é—®é¢˜1]
- [å¼•å¯¼é—®é¢˜2]
- [å¼•å¯¼é—®é¢˜3]

### ğŸš« ç¦æ­¢çš„æ’ç‰ˆè¡Œä¸º

- âŒ å°† 1. 2. 3. å†™åœ¨åŒä¸€è¡Œ
- âŒ æ ‡é¢˜åä¸æ¢è¡Œç›´æ¥æ¥å†…å®¹
- âŒ æ•´æ®µæ–‡å­—æ²¡æœ‰ä»»ä½•æ¢è¡Œ
- âŒ ä½¿ç”¨ ã€æ¥æº:xxxã€‘ æˆ–å…¶ä»–è§’æ ‡

## ç¦æ­¢äº‹é¡¹
- ä¸è¦æ¨èå…·ä½“å¤„æ–¹è¯åç§°æˆ–å‰‚é‡
- ä¸è¦åšå‡ºç¡®è¯Šæ€§åˆ¤æ–­ï¼ˆå¦‚"æ‚¨çš„å®å®å¾—äº†XX"ï¼‰
- ä¸è¦ä½¿ç”¨ç»å¯¹åŒ–æ‰¿è¯º"""

    def format_with_citations(self, answer: str, sources: List[KnowledgeSource]) -> str:
        """
        æ ¼å¼åŒ–ç­”æ¡ˆï¼Œæ¸…ç†æ¥æºæ ‡è®°å¹¶è¿”å›çº¯å‡€ç­”æ¡ˆ

        Args:
            answer: åŸå§‹ç­”æ¡ˆ
            sources: æ¥æºåˆ—è¡¨

        Returns:
            str: æ¸…ç†åçš„ç­”æ¡ˆï¼ˆä¸æ‹¼æ¥æ¥æºåˆ°æ­£æ–‡ï¼‰
        """
        # æ¸…ç† LLM å¯èƒ½æ®‹ç•™çš„æ¥æºæ ‡è®°
        clean_answer = re.sub(r'ã€æ¥æº:[^ã€‘]+ã€‘', '', answer).strip()
        return clean_answer

    def get_sources_metadata(self, sources: List[KnowledgeSource]) -> List[Dict[str, Any]]:
        """
        å•ç‹¬è¿”å›æ¥æºå…ƒæ•°æ®ï¼ˆä¾›å‰ç«¯ä½¿ç”¨ï¼‰

        Args:
            sources: æ¥æºåˆ—è¡¨

        Returns:
            List[Dict[str, Any]]: æ¥æºå…ƒæ•°æ®åˆ—è¡¨
        """
        return [
            {
                "id": s.metadata.get("id", "unknown"),
                "title": s.metadata.get("title", "æœªçŸ¥"),
                "source": s.source
            }
            for s in sources
        ]

    def get_entry_by_id(self, entry_id: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®IDè·å–çŸ¥è¯†åº“æ¡ç›®ï¼ˆç”¨äºç‚¹å‡»è§’æ ‡æŸ¥çœ‹åŸæ–‡ï¼‰

        Args:
            entry_id: æ¡ç›®ID

        Returns:
            Optional[Dict[str, Any]]: æ¡ç›®å†…å®¹
        """
        for entry in self.knowledge_base:
            if entry.get('id') == entry_id:
                return entry
        return None


# åˆ›å»ºå…¨å±€å®ä¾‹
rag_service = RAGService()
