"""
RAG æ£€ç´¢å‡†ç¡®æ€§æµ‹è¯•å¥—ä»¶

æµ‹è¯•å†…å®¹ï¼š
1. æ£€ç´¢è´¨é‡æµ‹è¯• (test_retrieval_quality)
   - éªŒè¯å…³é”®è¯å‘½ä¸­ç‡
   - éªŒè¯ç›¸å…³æ€§åˆ†æ•°

2. å»¶è¿ŸåŸºå‡†æµ‹è¯• (test_latency_benchmark)
   - æµ‹é‡æ£€ç´¢å¹³å‡è€—æ—¶

è¿è¡Œæ–¹å¼ï¼š
    pytest tests/test_rag_accuracy.py -v -s

å‰ç½®æ¡ä»¶ï¼š
    ç¡®ä¿å·²è¿è¡Œ scripts/migrate_to_chroma.pyï¼ŒChromaDB ä¸­æœ‰æ•°æ®
"""
import json
import time
import asyncio
from pathlib import Path
from typing import List, Dict, Any

import pytest
import pytest_asyncio
from loguru import logger

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.services.rag_service import RAGService


# ============ Fixtures ============

@pytest.fixture(scope="module")
def golden_dataset() -> Dict[str, Any]:
    """åŠ è½½é‡‘æ ‡å‡†æ•°æ®é›†"""
    fixture_path = Path(__file__).parent / "fixtures" / "golden_dataset.json"

    if not fixture_path.exists():
        pytest.skip(f"é‡‘æ ‡å‡†æ•°æ®é›†ä¸å­˜åœ¨: {fixture_path}")

    with open(fixture_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print(f"\nğŸ“¦ åŠ è½½é‡‘æ ‡å‡†æ•°æ®é›†: {len(data.get('test_cases', []))} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    return data


@pytest.fixture(scope="module")
def rag_service():
    """åˆå§‹åŒ– RAG æœåŠ¡å®ä¾‹"""
    service = RAGService()

    # éªŒè¯æœåŠ¡å¯ç”¨æ€§
    print(f"\nğŸ”§ åˆå§‹åŒ– RAG æœåŠ¡...")
    print(f"   - use_chromadb: {service._use_chromadb}")
    print(f"   - knowledge_base size: {len(service.knowledge_base)}")

    yield service


# ============ æµ‹è¯•ç±» ============

class TestRetrievalQuality:
    """æ£€ç´¢è´¨é‡æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_retrieval_quality(
        self,
        rag_service: RAGService,
        golden_dataset: Dict[str, Any]
    ):
        """
        æµ‹è¯•æ£€ç´¢è´¨é‡

        éªŒè¯ç‚¹ï¼š
        1. å…³é”®è¯å‘½ä¸­ç‡ï¼šè¿”å›çš„æ–‡æ¡£ä¸­åº”åŒ…å«è‡³å°‘ä¸€ä¸ªæœŸæœ›å…³é”®è¯
        2. ç›¸å…³æ€§åˆ†æ•°ï¼šè¿”å›çš„ score åº”é«˜äº min_score
        """
        test_cases = golden_dataset.get("test_cases", [])
        results_summary = []

        print("\n" + "=" * 60)
        print("ğŸ§ª RAG æ£€ç´¢è´¨é‡æµ‹è¯•")
        print("=" * 60)

        passed = 0
        failed = 0

        for case in test_cases:
            case_id = case["id"]
            query = case["query"]
            expected_keywords = case.get("expected_keywords", [])
            min_score = case.get("min_score", 0.5)

            print(f"\nğŸ“ [{case_id}] {query}")
            print("-" * 50)

            # æ‰§è¡Œæ£€ç´¢
            start_time = time.time()
            results = await rag_service.retrieve(query, top_k=3)
            elapsed = (time.time() - start_time) * 1000

            if not results:
                print(f"   âš ï¸  æœªè¿”å›ä»»ä½•ç»“æœ (çŸ¥è¯†åº“å¯èƒ½æ— ç›¸å…³å†…å®¹)")
                # æ ‡è®°ä¸ºè·³è¿‡è€Œéå¤±è´¥ï¼ˆçŸ¥è¯†åº“å¯èƒ½ä¸åŒ…å«ç›¸å…³ä¸»é¢˜ï¼‰
                results_summary.append({
                    "id": case_id,
                    "passed": True,
                    "skipped": True,
                    "reason": "no_results"
                })
                continue

            # æ‰“å°æ£€ç´¢ç»“æœ
            print(f"   â±ï¸  è€—æ—¶: {elapsed:.1f}ms")
            print(f"   ğŸ“‹ è¿”å› {len(results)} æ¡ç»“æœ:")

            keyword_hit = False
            score_ok = True

            for i, result in enumerate(results):
                title = result.metadata.get("title", "N/A")
                score = result.score
                content_preview = result.content[:60] + "..." if len(result.content) > 60 else result.content

                print(f"      [{i+1}] {title} (score={score:.3f})")
                print(f"          {content_preview}")

                # æ£€æŸ¥å…³é”®è¯å‘½ä¸­
                content_lower = result.content.lower()
                for keyword in expected_keywords:
                    if keyword.lower() in content_lower:
                        keyword_hit = True
                        print(f"          âœ… å‘½ä¸­å…³é”®è¯: {keyword}")
                        break

                # æ£€æŸ¥åˆ†æ•°
                if score < min_score:
                    score_ok = False

            # æ–­è¨€ 1: å…³é”®è¯å‘½ä¸­
            if expected_keywords:
                assert keyword_hit, (
                    f"[{case_id}] å…³é”®è¯æœªå‘½ä¸­! "
                    f"æœŸæœ›å…³é”®è¯: {expected_keywords}"
                )

            # æ–­è¨€ 2: åˆ†æ•°æ£€æŸ¥ (Top-1 å¿…é¡»è¾¾æ ‡)
            if results:
                top1_score = results[0].score
                assert top1_score >= min_score, (
                    f"[{case_id}] Top-1 åˆ†æ•°è¿‡ä½: {top1_score:.3f} < {min_score}"
                )

            print(f"   âœ… é€šè¿‡")
            passed += 1
            results_summary.append({
                "id": case_id,
                "passed": True,
                "elapsed_ms": elapsed,
                "top1_score": results[0].score if results else 0,
                "keyword_hit": keyword_hit
            })

        # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 60)
        print(f"   é€šè¿‡: {passed}/{len(test_cases)}")
        print(f"   å¤±è´¥: {failed}/{len(test_cases)}")

        if results_summary:
            avg_elapsed = sum(r.get("elapsed_ms", 0) for r in results_summary if r.get("passed")) / max(passed, 1)
            avg_score = sum(r.get("top1_score", 0) for r in results_summary if r.get("passed")) / max(passed, 1)
            print(f"   å¹³å‡è€—æ—¶: {avg_elapsed:.1f}ms")
            print(f"   å¹³å‡åˆ†æ•°: {avg_score:.3f}")

        print("=" * 60)

        # æœ€ç»ˆæ–­è¨€
        assert failed == 0, f"æœ‰ {failed} ä¸ªæµ‹è¯•ç”¨ä¾‹å¤±è´¥"


class TestLatencyBenchmark:
    """å»¶è¿ŸåŸºå‡†æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_latency_benchmark(self, rag_service: RAGService):
        """
        æµ‹è¯•æ£€ç´¢å»¶è¿Ÿ

        éªŒè¯ç‚¹ï¼š
        - 5 æ¬¡æŸ¥è¯¢çš„å¹³å‡è€—æ—¶åº”ä½äº 1.0 ç§’
        """
        test_queries = [
            "å®å®å‘çƒ§æ€ä¹ˆåŠ",
            "è…¹æ³»æ€ä¹ˆæŠ¤ç†",
            "å’³å—½æœ‰ç—°",
            "æ³°è¯ºæ—ç”¨é‡",
            "æ‘”å€’å¤´éƒ¨"
        ]

        print("\n" + "=" * 60)
        print("â±ï¸  å»¶è¿ŸåŸºå‡†æµ‹è¯•")
        print("=" * 60)

        latencies = []

        for i, query in enumerate(test_queries, 1):
            start_time = time.time()
            results = await rag_service.retrieve(query, top_k=3)
            elapsed = time.time() - start_time
            latencies.append(elapsed)

            print(f"   [{i}] '{query[:20]}...': {elapsed*1000:.1f}ms ({len(results)} æ¡ç»“æœ)")

        avg_latency = sum(latencies) / len(latencies)
        max_latency = max(latencies)
        min_latency = min(latencies)

        print("\nğŸ“Š å»¶è¿Ÿç»Ÿè®¡:")
        print(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency*1000:.1f}ms")
        print(f"   æœ€å¤§å»¶è¿Ÿ: {max_latency*1000:.1f}ms")
        print(f"   æœ€å°å»¶è¿Ÿ: {min_latency*1000:.1f}ms")

        # è­¦å‘Šé˜ˆå€¼
        if avg_latency > 1.0:
            print(f"   âš ï¸  WARNING: å¹³å‡å»¶è¿Ÿè¶…è¿‡ 1.0 ç§’!")
        else:
            print(f"   âœ… å»¶è¿Ÿæ­£å¸¸")

        print("=" * 60)

        # æ–­è¨€ï¼šå¹³å‡å»¶è¿Ÿåº”ä½äº 2.0 ç§’ï¼ˆè€ƒè™‘åˆ°æ¨¡å‹åŠ è½½ç­‰é¦–æ¬¡æ“ä½œï¼‰
        assert avg_latency < 2.0, f"å¹³å‡å»¶è¿Ÿè¿‡é«˜: {avg_latency:.2f}s"


class TestEdgeCases:
    """è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_empty_query(self, rag_service: RAGService):
        """æµ‹è¯•ç©ºæŸ¥è¯¢"""
        results = await rag_service.retrieve("", top_k=3)
        # ç©ºæŸ¥è¯¢åº”è¿”å›ç©ºç»“æœæˆ–ä½ç›¸å…³æ€§ç»“æœ
        print(f"\n   ç©ºæŸ¥è¯¢è¿”å› {len(results)} æ¡ç»“æœ")
        assert isinstance(results, list)

    @pytest.mark.asyncio
    async def test_very_long_query(self, rag_service: RAGService):
        """æµ‹è¯•è¶…é•¿æŸ¥è¯¢"""
        long_query = "å®å®" * 100  # 200 ä¸ªå­—ç¬¦
        results = await rag_service.retrieve(long_query, top_k=3)
        print(f"\n   è¶…é•¿æŸ¥è¯¢è¿”å› {len(results)} æ¡ç»“æœ")
        assert isinstance(results, list)

    @pytest.mark.asyncio
    async def test_special_characters(self, rag_service: RAGService):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦æŸ¥è¯¢"""
        special_query = "å®å®!@#$%å‘çƒ§"
        results = await rag_service.retrieve(special_query, top_k=3)
        print(f"\n   ç‰¹æ®Šå­—ç¬¦æŸ¥è¯¢è¿”å› {len(results)} æ¡ç»“æœ")
        assert isinstance(results, list)

    @pytest.mark.asyncio
    async def test_age_filter(self, rag_service: RAGService):
        """æµ‹è¯•å¹´é¾„è¿‡æ»¤"""
        # æŸ¥è¯¢æ–°ç”Ÿå„¿ç›¸å…³é—®é¢˜
        results = await rag_service.retrieve(
            "æ–°ç”Ÿå„¿å‘çƒ§",
            top_k=3,
            filters={"age_months": 1}
        )
        print(f"\n   å¹´é¾„è¿‡æ»¤æŸ¥è¯¢è¿”å› {len(results)} æ¡ç»“æœ")
        assert isinstance(results, list)


# ============ è¿è¡Œå…¥å£ ============

if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
